<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data | Deltanomics</title>
    <link>/tags/data/</link>
      <atom:link href="/tags/data/index.xml" rel="self" type="application/rss+xml" />
    <description>Data</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©2020</copyright><lastBuildDate>Sun, 26 Apr 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu92a9dc9639b79cadf266306904fb01f6_516996_512x512_fill_lanczos_center_2.png</url>
      <title>Data</title>
      <link>/tags/data/</link>
    </image>
    
    <item>
      <title>Covid-19 and Rural Areas in the U.S.</title>
      <link>/post/covid-19-rural-deltanomics/</link>
      <pubDate>Sun, 26 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/covid-19-rural-deltanomics/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Now that we are square in the middle of the Covid-19 pandemic, I thought it might be beneficial to look at some statistics associated with the number of cases.
We’ll differentiate our analysis by focusing on cases of Covid-19 in rural areas of the U.S.
There are couple of reasons for this: mainly, rural analytics is my speciality, so while I don’t know much about the virus, I do know some about rural societies and economies; we can easily find pertinant data on rural counties; and, we can utilise some cool built-in R functions to help us along the way.
Before we go on it’s important to note that:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I am not a medical doctor or specialist in viral diseases.&lt;/li&gt;
&lt;li&gt;This post is meant to be a learning resource for people interested in looking at the pandemic from a rural perspective.&lt;/li&gt;
&lt;li&gt;Any potential interesting findings must be further investigated before any judements can be made.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data&lt;/h1&gt;
&lt;p&gt;The data for this post comes from two places: Covid-19 cases from the &lt;a href=&#34;https://github.com/nytimes/covid-19-data&#34;&gt;New York Times github&lt;/a&gt;; and the &lt;a href=&#34;https://www.ers.usda.gov/data-products/rural-urban-continuum-codes/&#34;&gt;USDA Rural-Urban Classification Codes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Datascientists at The New York Times have been collating data on number of cases of Covid-19 by county in the U.S..
It is available at their github page, which means that one can easily access and update the data through &lt;em&gt;pull requests&lt;/em&gt;.
The data can also be downloaded and saved to a local drive.&lt;/p&gt;
&lt;p&gt;To get a rural understanding of Covid-19 cases, we’ll use the USDA data on rural-urban classification of U.S. counties, which can be downloaded using the hyperlink above.
Many countries have geographical classifcations for rural and urban spaces.
Usually, a low-level geography is chosen that spans an entire country.
A continuum of rural-urban is used to describe each geographical area that goes from very urban to very rural (though not using those specific labels).&lt;/p&gt;
&lt;p&gt;So we’ll use those two datasets, join them together and investigate how many cases of Covid-19 are found in rural areas within the U.S..&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis&lt;/h1&gt;
&lt;div id=&#34;libraries-and-themes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Libraries and themes&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Tidyverse&lt;/code&gt; packages will be used to do most of the heavy lifting.
I created a logo for Deltanomics - it’s the icon that shows up in your browser tab - that I’m going to stitch on to the plots.
Of course, you don’t have to do for your analysis, and you can leave it off by omitting the code at the end. The &lt;code&gt;cowplot&lt;/code&gt; package is used to help stitch the logo to the &lt;code&gt;ggplot2&lt;/code&gt; plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(janitor)
library(scales)
library(cowplot)

# USDA Rural-Urban classification codes


post_theme &amp;lt;- function(...) {
  theme(
    text = 
      element_text(
        color = &amp;#39;black&amp;#39;,
         family = &amp;#39;serif&amp;#39;),
    axis.text = 
      element_text(
        color = &amp;#39;black&amp;#39;),
    axis.text.x = 
      element_text(angle = 45, 
                   hjust = 1),
    panel.background = 
      element_blank(),
    axis.line.x = 
      element_line(
        color = &amp;#39;black&amp;#39;),
    axis.ticks = element_blank(),
    plot.margin = margin(.75, .75, .75, .75, &amp;#39;cm&amp;#39;),
    plot.caption = 
      element_text(hjust = 0,
                                face = &amp;quot;italic&amp;quot;),
    plot.title = 
      element_text(
        face = &amp;#39;bold&amp;#39;),
    plot.subtitle = 
      element_text(face = &amp;#39;bold&amp;#39;),
    plot.title.position = &amp;quot;plot&amp;quot;,
    plot.caption.position =  &amp;quot;plot&amp;quot;, 
    strip.background = 
      element_blank(), 
    strip.text = 
      element_text(
        face = &amp;#39;bold&amp;#39;)
  ) +
    theme(...) # this bit allows us to make changes using this same function instead of calling two theme functions.
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# I created a new R-project to house the Covid-19 data in my Documents directory. 
covid_county &amp;lt;- read_csv(&amp;#39;~/Documents/R/covid-19-data/us-counties.csv&amp;#39;)

# Pull in the USDA data from a directory I created in my cloud storage.
rural_urban &amp;lt;-
  read_csv(&amp;#39;~/OneDrive - SRUC/Data/usda/ruralurbancodes2013.csv&amp;#39;) %&amp;gt;%
  select(fips,
         rucc_2013,
         desc = description)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we’ve got the data loaded from Covid-19 and USDA Rural-Urban Classifications, we are going to use some of R’s base functionality.
R has two functions that help with general data analysis and joins: &lt;code&gt;state.name&lt;/code&gt;, which has all 50 U.S. state names; and, &lt;code&gt;state.region&lt;/code&gt;, which has the 50 U.S. state’s categorised into geographical regions.&lt;br /&gt;
We’ll use this two functions in a &lt;code&gt;tibble&lt;/code&gt; to help join the Covid-19 data with the Rural-Urban Classifications.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;state_region &amp;lt;-
  tibble(state = state.name,
         region = state.region)

covid_region &amp;lt;-
  covid_county %&amp;gt;%
  left_join(rural_urban,
            by = &amp;#39;fips&amp;#39;) %&amp;gt;%
  left_join(state_region,
            by = &amp;#39;state&amp;#39;) %&amp;gt;%
  mutate(week =  
           floor_date(date,
                      &amp;#39;week&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have our working dataframe called &lt;code&gt;covid_region&lt;/code&gt;. It has the following variable names: date, county, state, fips, cases, deaths, rucc_2013, desc, region, week.
We’ll use the &lt;code&gt;description&lt;/code&gt; variable to filter out rural counties only.
There are two classifications of rural areas - those that are adjacent to more metro places and those that are not.
Those that are not adjacent to metro areas are adjecent other rural areas, which makes them somewhat more remote, as people living there need to travel further to get to service centres.&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;group_by&lt;/code&gt;/&lt;code&gt;summarise&lt;/code&gt; functionality from &lt;code&gt;dplyr&lt;/code&gt; to find the sum of Covid-19 cases and deaths by each week, region and for both rural classifications.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weekly_regions &amp;lt;- covid_region %&amp;gt;%
  filter(str_detect(desc,
                    &amp;#39;rural&amp;#39;)) %&amp;gt;% 
  group_by(week, region, desc) %&amp;gt;%
  summarise(cases =
              sum(cases, na.rm = T),
            deaths =
              sum(deaths, na.rm = T)) %&amp;gt;%
  ungroup() %&amp;gt;%
  gather(key,
         value,-c(week,
                  region,
                  desc))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re going to make a fancy-looking plot for this post, something that you might like to share on social media or include in a work report.
To help clean up the plot a bit, we’ll use a few approaches that are layed out in the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# each label for the x-axis which we&amp;#39;ll use to make some nice looking data labels.
week_n &amp;lt;- weekly_regions %&amp;gt;% 
  count(week) %&amp;gt;% 
  pull(week)

# we&amp;#39;ll use scale_color_manual with our own color choice 
col_v &amp;lt;- c(&amp;#39;#3E4A89FF&amp;#39;, &amp;#39;#FDE725FF&amp;#39;)
names(col_v) &amp;lt;- unique(weekly_regions$desc)

# a simple label_wrap function for the legend
label_wrap &amp;lt;- function(x, n = 25){
  paste0(str_wrap(x, n), &amp;#39;\n&amp;#39;)
}

# date labels that use drops today&amp;#39;s date into the caption of the plot.
today_date &amp;lt;- 
  as_date(Sys.time()) %&amp;gt;% 
  format(&amp;#39;%d %B, %Y&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK, now we’ll create the main ggplot that uses &lt;code&gt;facet_wrap&lt;/code&gt; to look each region in the U.S. overtime.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weekly_regional_gg &amp;lt;- 
  weekly_regions %&amp;gt;%
  filter(key == &amp;#39;cases&amp;#39;) %&amp;gt;% 
  ggplot(aes(week, 
         value, 
         group = desc)) +
  geom_line(size = 1.25, 
            aes(color = desc))+
  geom_point(size = 4, 
             color = &amp;#39;grey90&amp;#39;)+
  geom_point(size = 3.5, 
             aes(color = desc))+
  scale_x_date(breaks = week_n, date_labels = &amp;#39;%d-%b&amp;#39;)+
  scale_y_continuous(labels = comma) +
  scale_color_manual(
    values = col_v, 
    labels = label_wrap)+
  facet_wrap( ~ region) +
  post_theme() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we’ll add the labels and annotations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weekly_regional_gg &amp;lt;- 
  weekly_regional_gg +  
  labs(
    title = &amp;#39;Weekly Total U.S. COVID-19 Cases by Region in Rural Counties&amp;#39;,
    subtitle = str_wrap(&amp;#39;SOURCE: The New York Times, based on reports from state and local health agencies &amp;amp; the USDA Rural-Urban Continuum Codes (2013).&amp;#39;, 115),
    x = &amp;#39;\nWeek of&amp;#39;,
    y = &amp;#39;Total&amp;#39;,
    color = str_wrap(&amp;#39;USDA Rural-Urban Continuum Codes (2013)&amp;#39;, 25), 
    fill = str_wrap(&amp;#39;USDA Rural-Urban Continuum Codes (2013)&amp;#39;, 25),
    caption = str_c(&amp;#39;By Elliot Meador, PhD; @Elliot_Meador\nNOTE: Last week is likely not yet complete.\nProduced &amp;#39;, today_date))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, for this plot, let’s stitch the Deltanomics logo on the bottom right of the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Now we start the logo stitching!

(final_plot &amp;lt;- ggdraw() +
  draw_plot(weekly_regional_gg)+
  draw_image(&amp;quot;~/Documents/R/deltanomics/themes/hugo-academic/assets/images/icon.png&amp;quot;, 
             scale = .125, 
             x = .4, 
             y = -.4)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-26-covid-19-and-rural-areas-in-the-u-s_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Networks from survey data: Creating mock data</title>
      <link>/post/networks-from-survey-data-creating-mock-data/</link>
      <pubDate>Fri, 27 Mar 2020 16:25:07 +0000</pubDate>
      <guid>/post/networks-from-survey-data-creating-mock-data/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;why-create-a-new-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why create a new dataset?&lt;/h1&gt;
&lt;p&gt;I’d like to do a series of posts looking at social network analysis using primary data (i.e. data collected by yourself.).
There are a lot of different examples of when you might want to use a survey to collect data for use in analysing social networks.
But that’s for another time.&lt;/p&gt;
&lt;p&gt;The purpose of this post is to create a new dataset that can be used in practising social network analysis in future posts.
Creating a new dataset in R has a lot of useful advantages.
The biggest advantage is that we will have a single dataset that can be used in all future examples when learning SNA with surveys.&lt;/p&gt;
&lt;p&gt;Creating a new dataset is also a great learning opportunity because we will &lt;em&gt;reverse engineer&lt;/em&gt; a dataset around specific modelling, correlations and otherwise interesting easter-eggs that we can use as learning opportunities in future posts.
We will rely on the power of probability statistics to help us get there.
And as we make decisions about how to structure our dataset, we’ll learn some important aspects of social network analysis and general data science.
We’ll save this for the end though.
So, let’s get started!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-new-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building a new dataset&lt;/h1&gt;
&lt;p&gt;As with most posts on Deltanomics, we’ll use a tidy framework. So, that means loading &lt;code&gt;tidyverse&lt;/code&gt;, and we’ll go ahead and load our other SNA workhorse packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For a tidy framework
library(tidyverse)
library(glue)
library(scales)

# Our graphing libraries
library(igraph)
library(tidygraph)
library(ggraph)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;an-edgelist&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An edgelist&lt;/h2&gt;
&lt;p&gt;The first thing we need to do is create an &lt;a href=&#34;https://en.wikipedia.org/wiki/Edge_list&#34;&gt;edgelist&lt;/a&gt; structure in our data.
Really anything can be used as an edgelist as it’s just two columns that represent an edge is meant to be drawn between adjacent cells.
A typical use of surveys in SNA is to look at how information flows between two people and the influence that the information has on sustainable behaviours.
So let’s create two columns that would reasonably collect that type of information.&lt;/p&gt;
&lt;div id=&#34;respondent-name&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Respondent name&lt;/h3&gt;
&lt;p&gt;First, we need a column for the respondent’s name or identification.
This column length will be the first and primary argument in our function to allow us to create datasets of any size we choose.&lt;/p&gt;
&lt;p&gt;For this, let’s use one of my favourite packages &lt;code&gt;randomNames&lt;/code&gt; to generate some realistic names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(randomNames)

create_sna_data &amp;lt;- 
tibble( # let&amp;#39;s pull 100 random names to start
    resp_name = randomNames(100,
                  which.names = &amp;#39;both&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;information-holder&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Information holder&lt;/h3&gt;
&lt;p&gt;Next, we’ll create a column that holds the name of whom the respondent goes to for information.
We want our social network to be &lt;em&gt;complete&lt;/em&gt;; meaning that every node in the graph will attribute data.
To ensure this happens, we need to take special care that all of the possible nodes are also respondents.
In short, the second column of the edgelist needs to be completely contained within the first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1. Make a disconnect graph
g &amp;lt;- make_empty_graph() %&amp;gt;% 
  add_vertices(2)

# 2. Run a while loop to ensure that a connected
# graph is created -- this will help smooth over some of the graphing functions for later. 
# 
while (is.connected(g)== FALSE) {

  g &amp;lt;- create_sna_data %&amp;gt;% 
    mutate(info_one = sample(
      sample(resp_name, 80), # create 2nd column
      nrow(.), T)) %&amp;gt;%       # as subset of the
    as_tbl_graph()           # first.
}

# send it back to the original name
create_sna_data &amp;lt;-  g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s take a look at how the social network contained within the data looks like.
&lt;img src=&#34;/post/2020-03-27-networks-from-survey-data-creating-mock-data_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;960&#34; /&gt;
The network should loosely resemble a sparsely connected sociogram, and it should serve our purposes well.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;node-edge-attributes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Node &amp;amp; edge attributes&lt;/h2&gt;
&lt;p&gt;Now that we have our edge list as the first two columns of the data set, we can start to add some node and edge attributes. However, we can’t just randomly create new variables and values because we want a dataset that resembles what we might find in the real world. This means certain variables should be related or correlated with one another. And, because we’re interested in network analysis, a node’s position in the network should also influence their values in key columns. To achieve this, we’ll need to &lt;em&gt;reverse&lt;/em&gt; engineer the values based on some graph analysis.&lt;/p&gt;
&lt;div id=&#34;node-attributes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Node attributes&lt;/h3&gt;
&lt;p&gt;We’ll do some rapid-fire node correlations with some key socio-economic variables.&lt;/p&gt;
&lt;div id=&#34;income-category&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Income category&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_sna_data &amp;lt;- create_sna_data %&amp;gt;% 
  mutate(income_pre_tax = map_chr(degree(create_sna_data), function(x){
  
    # random normal using degree as the mean
    # and a standard deviation of 2.5
    random_norm &amp;lt;- rnorm(n = 1, 
        mean = x, 
        sd = sample(2.5, 1, F))
  
  dollar(abs(random_norm)*15000, 
         prefix = &amp;#39;£&amp;#39;)
}))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our dataset has a lot of randomness to it, so it’s impossible to tell what the correlation is. But, it should at least be positive and somewhat linear. There aren’t likely to be many nodes that have the maximum number of degrees, so the variance should drop off as the degree increases (but this isn’t a guarantee!).&lt;/p&gt;
&lt;p&gt;A boxplot of showing degree and income is shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-27-networks-from-survey-data-creating-mock-data_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
So, the theoretical people in our dataset with more connections to others should make more money, something that, could conceivably be true.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;neighbourhood-influence&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Neighbourhood influence&lt;/h4&gt;
&lt;p&gt;A common question in network analysis is: do nodes behave differently when they are connected to certain nodes. It’s like the old adage ~ &lt;em&gt;if you lie down with dogs you’ll get up with fleas&lt;/em&gt;. For this, we’ll pick out some random nodes and have their neighbourhoods adopt a similar value for a question like: do you buy the majority of your fruit and veg from a farmers market?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;influencers_df &amp;lt;- map_df(1:10, function(x){

  # pull a random node name
node. &amp;lt;- sample(V(create_sna_data)$name, 1)

  # get the node id, because to_local_neighborhood requires a numeric identifier (this is due to igraph).

 node_id. &amp;lt;- match(node., V(create_sna_data)$name)
  
 # pull the neighbourhoods of each node from above. 
neighbours. &amp;lt;- create_sna_data %&amp;gt;% 
  to_local_neighborhood(node = node_id., 
                        order = 1) %&amp;gt;% 
    .[[1]] %&amp;gt;% 
    as_tibble() %&amp;gt;% 
    pull(name)

 # create a tibble of both values for use in the next step

tibble(neighours. = neighbours.,
       centre = rep(node., length(neighbours.)))

}) 
# create new variable for each value returned above.

create_sna_data &amp;lt;- 
  create_sna_data %&amp;gt;% 
  mutate(buy_farm_mark = case_when(
    name %in% influencers_df$centre ~ &amp;#39;Every meal&amp;#39;,
    name %in% influencers_df$neighours. ~ &amp;#39;Most meals&amp;#39;,
    T ~ &amp;#39;Hardly any meals&amp;#39;
  ), 
  buy_farm_mark = factor(buy_farm_mark, 
                            levels = c(&amp;#39;Every meal&amp;#39;, 
                                       &amp;#39;Most meals&amp;#39;, 
                                       &amp;#39;Hardly any meals&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That was a bit verbose and somewhat complicated, but it will be worth it. Let’s take a look below to see how it looks in our new data.
&lt;img src=&#34;/post/2020-03-27-networks-from-survey-data-creating-mock-data_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;community-influence&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Community influence&lt;/h4&gt;
&lt;p&gt;We’ll use a community detection algorithm for the last node attribute for our dataset. This one is a bit easier as we’ll just create a new variable using the &lt;code&gt;group_infomap&lt;/code&gt; function from &lt;code&gt;tidygraph&lt;/code&gt;/&lt;code&gt;igraph&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_sna_data &amp;lt;- create_sna_data %&amp;gt;% 
  to_undirected() %&amp;gt;% 
  mutate(cows_on_farm = 
           as.factor(group_infomap()))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below illustrates the communities detected by &lt;code&gt;group_infomap&lt;/code&gt;. The only thing we’ve done here is to rename the variable. Easy enough!
&lt;img src=&#34;/post/2020-03-27-networks-from-survey-data-creating-mock-data_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’ll now add edge attributes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;edge-attributes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Edge attributes&lt;/h3&gt;
&lt;p&gt;Edge attributes won’t be as complicated as node attributes for as we’ve aleady identified the relationship between nodes (edges). We’ll just need to think about a variable that would makes sense for trustful communities. One could be that number of cows is related to higher levels of trust (not super likely in the real world, but anything’s possible!). It’s an easy edge attribute to calculate so let’s do that one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_sna_data &amp;lt;- create_sna_data %&amp;gt;% 
  mutate(trust_score = round(
              rescale(
              as.numeric(cows_on_farm), 
              c(1, 10))))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;back-to-a-tibble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Back to a tibble&lt;/h2&gt;
&lt;p&gt;We’ve been workig with a &lt;code&gt;tidygraph&lt;/code&gt; object for most the post. We’ll want to create a &lt;code&gt;tibble&lt;/code&gt; for our purposes. Remember, the goal is to create a mock survey dataset that we can use in the future to learn SNA. So it should look authentic. Let’s do that now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;name_id_df &amp;lt;- create_sna_data %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  transmute(name, 
            value = row_number())

create_sna_data &amp;lt;- create_sna_data %&amp;gt;% 
  activate(edges) %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  gather(key, value) %&amp;gt;% 
  left_join(name_id_df) %&amp;gt;% 
  split(.$key) %&amp;gt;% 
  bind_cols() %&amp;gt;% 
  select(resp_name = name, 
         recieve_info = name1) %&amp;gt;% 
  bind_cols(create_sna_data %&amp;gt;% 
              as_tibble() %&amp;gt;% 
              select(-name))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All right, that’s it! We can look at our data below; hopefully, it looks like something we might collect in the future for SNA research.&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-13&#34;&gt;Table 1: &lt;/span&gt;Our mock dataset for SNA
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
resp_name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
recieve_info
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
income_pre_tax
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
buy_farm_mark
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
cows_on_farm
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
trust_score
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Roberts, Nicole
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
el-Younis, Tayyiba
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
£6,389.69
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Hardly any meals
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Dixon, Lanasia
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Stirewalt, Sutter
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
£23,319.29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Every meal
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Warat, Calvin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Richardson, Chelsea
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
£27,098.81
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Most meals
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Chroneos, Samuel
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Ocampo, Ruth
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
£26,924.22
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Hardly any meals
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Lamichhane, Wesley
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
al-Shaheen, Husaam
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
£36,799.65
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Hardly any meals
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Loehr, Jamie
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Schmalz, Keiley
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
£73,347.91
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
Most meals
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;color: black !important;background-color: white !important;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
