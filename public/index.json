[{"authors":["admin"],"categories":null,"content":"Elliot Meador is a computational social scientist and data scientist working as a research fellow within the Rural Policy Centre at SRUC in Edinburgh, UK. His work focuses on mapping rural resilience in Scotland and understanding how policy impacts community resilience. Elliot\u0026rsquo;s research also looks at how information on sustainable agriculture practices is shared between farmers and how this influences behaviour change related to sustainable development. Elliot\u0026rsquo;s research spans North America, Europe and Africa.\n","date":1596844800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1596844800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Elliot Meador is a computational social scientist and data scientist working as a research fellow within the Rural Policy Centre at SRUC in Edinburgh, UK. His work focuses on mapping rural resilience in Scotland and understanding how policy impacts community resilience. Elliot\u0026rsquo;s research also looks at how information on sustainable agriculture practices is shared between farmers and how this influences behaviour change related to sustainable development. Elliot\u0026rsquo;s research spans North America, Europe and Africa.","tags":null,"title":"Elliot Meador PhD","type":"authors"},{"authors":[],"categories":["R","Development"],"content":"  knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE) Introduction Open source tools for development work allows for the power of big data, machine learning and AI to come together to benefit people who may need it most.\nOpen source tools are those with publicly available source code that can be downloaded or changed, completely free of charge.\nThere is a wide range of open source tools available these days: for instance, this blog is entirely written in the R programming language, which is free and open source. Other tools include Python, a very popular data science tool. Open source tools extend beyond programming languages. There are platforms that provide access to big, open data sets that can be freely used. A fantastic example of an open data tool that can be useful in development work is the OpenStreetMap (OSM).\nAccording to the OpenStreetMap website, “OpenStreetMap is a map of the world, created by people like you and free to use under an open license.”\nBasically, the OSM is an open map, in that regular people help identify infrastructure and other map objects for use in the map’s database. The OSM and its data can be accessed through an API, meaning that data can be pulled directly into R or other applications. The amount of really useful data one can pull from the OSM is an amazing asset to data science in a development context.\nTake a look at these resources for some more information:\n OpenStreetMap Wiki page\n A list of books on OpenStreetMap\n A very cool humanitarian project using OpenStreetMap\n  Getting data It’s straight forward to pull in data from the OSM for use in R with the osmdata by Mark Padgham, Bob Rudis, Robin Lovelace and Maëlle Salmon. The osmar package by Thomas Schlesinger and Manuel J. A. Eugster provides some extra R tools to work with the data.\n Approach In general, places that have high levels of civic engagement do better in successfully completing community development projects. Knowing this, let’s take a look at concentrations of community development centres across Scotland.\n Analysis Our analysis will centre on getting the features we are interested in, plotting them within context an perhaps adding more, relevant, features to help bring out any emerging data narratives. First we’ll need to load our libraries.\nLibraries First we need to we need to load our packages. We’re going to be working with spatial data, so we’ll load the sf package for that along with standard tidyverse packages for analysis.\nlibrary(tidyverse) library(scales) library(osmdata) library(sf) library(janitor)   Features We are interested in community development centres within the Scotland, specifically within the rural areas of the country. We can see all the available features using the available_features function. Each feature comes with a subset of tags, which are more specific places.\n Expanding to the islands We’ll use a looping approach to replicated the analysis for four islands and one area with a large island population (Argylle and Bute). Here is a list of the key places we are going to look at:\n Argyll and Bute Shetland Islands Outer Hebrides Orkney    An efficient workflow Getting an efficient workflow is critical to many data science projects. Workflows that aren’t efficient can cause unneeded delays in projects and are often the cause of underlying stress on the job. Knowing this, a good workflow for this type of project (mapping community development amenities) is to:\nIdentify the key geographic place boundaries that you want to work within and store them in a vector.\n Loop over the place vector to and pull in the amenity data, then store this in a list.\n Loop of the place vector and pull in the boundary data, then also store this in a list.\n  Our workflow keeps everything stored lists of the same geographic order. This will make sense as we work our way through the project as we will do most of our operations using loops. In R, looping is typically done with the apply family of functions. However, I opt to use purrrs version of the apply functions called map. I’d highly recommend using functional programming and the apply/purrr workflow as it allows you to utilise the full power of iterative programming!\nCommunity centres Data can be pulled directly from OpenStreetMap in R using only a few lines of code. The code below does the following:\ngetbb(.x) - gets a bounding box around a region identified in a string opq() - builds the query to the Overpass API add_osm_feature() - adds a feature to your Overpass API osmdata_sf() - turns the query into a simple features (sf) object.  The add_osm_feature is where we specify the amenity that we want. There are many, many different amenities that you can access with R and osmdata. I’d recommend that you have a look at the tags/features documentation to better understand how you can structure queries to access different information – it’s extremely powerful. We are interested in community centres; so we specifiy the key as amenity and the value as community_centre.\n## get a geographical box that contains ## Scottish islands scottish_places \u0026lt;- c( \u0026quot;argyll and bute\u0026quot;, \u0026quot;shetland\u0026quot;, \u0026quot;outer hebrides\u0026quot;, \u0026quot;orkney\u0026quot; ) scottish_places_ls \u0026lt;- scottish_places %\u0026gt;% map(~{ getbb(.x) %\u0026gt;% opq() %\u0026gt;% add_osm_feature( key = \u0026quot;amenity\u0026quot;, value = \u0026quot;community_centre\u0026quot;) %\u0026gt;% osmdata_sf() }) %\u0026gt;% set_names( make_clean_names( scottish_places ) ) scottish_places_ls \u0026lt;- map(1:4, ~{ scottish_places_ls[[.x]][[6]] %\u0026gt;% group_by(osm_id) %\u0026gt;% st_centroid() %\u0026gt;% ungroup() }) %\u0026gt;% set_names( make_clean_names( scottish_places ) )  Place boundaries Now we need to grab the boundary data for these places. Boundary data can be difficult to identify because places can sometimes go by many different name with differences in spelling. This seems to be a common occurrence when using geographic data and is likely the thinking behind things like FIPS codes in the US and other identification indeces. OpenStreetMap has unique id numbers that we can take advantage of as well. Take a look at the search function for OpenStreetMap search function for more information.\nOnce you’ve identified your geographical boundary of interest and spotted its ID number, the opq_osm_id function can be used to access the data from the Overpass API. We’re interested in the multipolygon data for the boundaries so we’ll using a loop and some base indexing to pull that information out specifically – we get a lot back from the original API request.\nplace_id \u0026lt;- c ( 1775685, 376677, 1959008, 3067412 ) place_name \u0026lt;- c( \u0026quot;argyll_and_bute\u0026quot;, \u0026quot;shetland\u0026quot;, \u0026quot;outer_hebrides\u0026quot;, \u0026quot;orkney\u0026quot; ) names(place_id) \u0026lt;- place_name place_boundaries_ls \u0026lt;- map(place_id, ~{ opq_osm_id( type = \u0026quot;relation\u0026quot;, id = .x ) %\u0026gt;% opq_string() %\u0026gt;% osmdata_sf() }) place_boundaries_ls \u0026lt;- map(1:4, ~{ place_boundaries_ls[[.x]][[8]] }) %\u0026gt;% set_names( names( place_boundaries_ls ) )  Feature cleaning - interesecting points The last thing we need to do is clean up those points that fall outside of the boundary line. This happens because we pull the community centre data through using a bounding box. Bounding boxes are, of course, square in shape. So we usually get data points that fall outside the area of interested – which is usually not a square! The st_join with an intersecting join (st_join) can help us shave off those extra points.\n## make the two geographies the same ## coordinate projection scottish_places_ls \u0026lt;- map2(scottish_places_ls, place_boundaries_ls, ~{ st_transform(.x, crs = st_crs(.y)) }) # find the points that intersect # with out boundaries data scottish_places_ls \u0026lt;- map2(scottish_places_ls, place_boundaries_ls, ~{ st_join(.x, .y, join = st_intersects) })   Results We’ve got more plans to do with the osmdata package in R – mainly using it to do some network analysis using the road network. So, we aren’t going to go into too much detail create visuals as we’ll do that in a later blog post. For now let’s create a decent looking (but not really publication quality) map of our results to help us spot check and problems. Then, later, we’ll pick it up and push forward with linking our data to the Scottish road network and creating some fantastic network diagrams of places in Scotland.\nVisualising the results For our visual we’ll create a simple ggplot map, which is pretty straightforward since we’re using sf objects. We’ll just look at Argyll and Bute community centres. Remember to look out for Part 2 of this series in the future where we will use osmdata and the Scottish road network to do some network mapping.\nggplot() + geom_sf( data = place_boundaries_ls$orkney, fill = \u0026quot;#73b896\u0026quot; ) + geom_sf( data = scottish_places_ls$orkney ) + geom_sf_text( data = scottish_places_ls$orkney, aes(label = str_wrap(name.x, 35)), size = 3.5, nudge_x = .0125, nudge_y = .0125, ) + theme_minimal() + theme(panel.background = element_rect(fill = \u0026quot;#b2bee0\u0026quot;)) + coord_sf() + labs(title = \u0026quot;Argyll and Bute Council\u0026quot;, subtitle = \u0026quot;Community Development Centres\u0026quot;, caption = \u0026quot;Made with OpenStreetMap\u0026quot;, x = NULL, y = NULL)   ","date":1612656000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612696370,"objectID":"0dfdd04e5a4ea7d10bac645897bdf7bf","permalink":"/post/open-street-map-leveraging-open-source-data-for-development-work/","publishdate":"2021-02-07T00:00:00Z","relpermalink":"/post/open-street-map-leveraging-open-source-data-for-development-work/","section":"post","summary":"knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE) Introduction Open source tools for development work allows for the power of big data, machine learning and AI to come together to benefit people who may need it most.\nOpen source tools are those with publicly available source code that can be downloaded or changed, completely free of charge.\nThere is a wide range of open source tools available these days: for instance, this blog is entirely written in the R programming language, which is free and open source.","tags":["dplyr","ggplot","sf"],"title":"Open Street Map: Leveraging open source data for development work","type":"post"},{"authors":[],"categories":["R","Rural","Election"],"content":"   Introduction The US Presidential vote count has nearly finished. Trump is fighting the results in court though appears to have lost, but a clear theme that’s coming out is the contrast of rural and urban voters. The theory seems to go: Trump voters for the most part come from rural areas, and the election is based on a rural vs urban debate. We can test this theory using some data science approaches - mainly, cleaning and merging data from several places into a useful format. For this post, we’ll be addressing the research question:\n Did Trump’s backing come from only rural areas and could it swing the election in his or future candidates favour?\n Let’s first load our libraries and get started!\n# For data wrangling/tables library(tidyverse) library(lubridate) library(ggtext) library(glue) library(scales) library(RColorBrewer) # For mapping and census data library(tigris) library(leaflet) library(leaflet.extras) library(maps) library(sf) library(widgetframe) library(htmlwidgets) library(htmltools) map \u0026lt;- purrr::map As always I like to use a custom ggplot function specified below.\n# a custom theme for ggplots post_theme \u0026lt;- function(...) { theme( text = element_text( color = \u0026#39;black\u0026#39;, family = \u0026#39;serif\u0026#39;), axis.text = element_text( color = \u0026#39;black\u0026#39;, size = 14), panel.background = element_blank(), axis.line.x = element_line( color = \u0026#39;black\u0026#39;), axis.ticks = element_blank(), plot.margin = margin(.75, .75, .75, .75, \u0026#39;cm\u0026#39;), plot.caption = element_text(hjust = 0, size = 15, face = \u0026quot;italic\u0026quot;), plot.title = element_text( face = \u0026#39;bold\u0026#39;), plot.subtitle = element_text(face = \u0026#39;bold\u0026#39;), plot.title.position = \u0026quot;plot\u0026quot;, plot.caption.position = \u0026quot;plot\u0026quot;, strip.background = element_blank(), strip.text = element_text( face = \u0026#39;bold\u0026#39;) ) + theme(...) # this bit allows us to make changes using this same function instead of calling two theme functions. }  The data Our process for analysis is a two-step process: first, we download and tidy data for the US 2020 presidential election results; then, we’ll join this data to the USDA Rural-Urban classifications. The resulting database will be used to investigate voting patterns between these classifications.\nElection data Election data comes from Fabio Votta’s Github page, under the repository for favstats/USElection2020-NYT-Results (visit https://github.com/favstats/USElection2020-NYT-Results). Fabio is a fantastic political data scientist. I’d recommend following him on Twitter (@favstats) as he shares a lot of great information on his work there. Fabio wrote a function that communicates with the New York Times data API to pull the data in a tidy format.\nI’ve already cloned into the favstats/USElection2020-NYT-Results repository and have the database of voting trends on my laptop. This repo has dated files with the election results as they come in. We’ll need a bit of code to help us identify the most recent version as it’s being updated. Below, is the code I used to input the data.\ndata_folders \u0026lt;- list.files(\u0026#39;~/Documents/R/USElection2020-NYT-Results/data/\u0026#39;) max_date_folder \u0026lt;- data_folders %\u0026gt;% str_subset(\u0026#39;^2020\u0026#39;) %\u0026gt;% max() results_2020 \u0026lt;- glue(\u0026#39;~/Documents/R/USElection2020-NYT-Results/data/{max_date_folder}\u0026#39;) %\u0026gt;% list.files(full.names = T) %\u0026gt;% str_subset(\u0026#39;presidential$\u0026#39;) %\u0026gt;% list.files(full.names = T) %\u0026gt;% str_subset(\u0026#39;.csv$\u0026#39;) %\u0026gt;% read_csv()  USDA Rural-Urban Classifications The USDA’s Rural-Urban Classification’s classify each US county along a rural-urban continuum. We’ve used these codes quite a lot when dealing with US county-level data. You can find this data here for download, https://www.ers.usda.gov/data-products/rural-urban-continuum-codes.aspx.\nIn addition, we’re going to use a few of R’s internal databases to help us out in the wrangling process, specifically we’ll use some data from the tigris package.\n In the process of writing this blog post I’ve discovered that Alaska uses electoral districts for counting votes - not counties. Therefore, for now, we’re going to leave Alaska out of the analysis because assigning a rural-urban code to electoral districts requires quite a bit of GIS work that’s outwith the scope of this post.\n # US county fips from tigris package fips_codes \u0026lt;- fips_codes %\u0026gt;% as_tibble() # Pull in the USDA data from a directory I created in my cloud storage. rural_urban \u0026lt;- read_csv( \u0026#39;~/OneDrive - SRUC/Data/usda/ruralurbancodes2013.csv\u0026#39; ) %\u0026gt;% as_tibble() %\u0026gt;% select(state, county = county_name, rucc_2013, desc = description)  Data join We need to join the election data, which has the county fips codes to identify each county, with the built-in county.fips database. Then, we have to join the election data with the new rural_urban_tidy data.\n# clean names will be used to help get joining data in format that will merge well clean_names \u0026lt;- function(x){ x \u0026lt;- str_to_lower(x) x \u0026lt;- str_remove_all(x, \u0026#39;county\u0026#39;) x \u0026lt;- str_remove_all(x, \u0026#39;[[:digit:]]\u0026#39;) x \u0026lt;- str_squish(x) x \u0026lt;- str_trim(x) x } fips_codes_tidy \u0026lt;- fips_codes %\u0026gt;% mutate(county = clean_names(county)) %\u0026gt;% mutate(fips = glue(\u0026#39;{state_code}{county_code}\u0026#39;)) rural_urban_tidy \u0026lt;- rural_urban %\u0026gt;% mutate(county = clean_names(county)) %\u0026gt;% left_join(fips_codes_tidy, by = c(\u0026#39;state\u0026#39;, \u0026#39;county\u0026#39;)) %\u0026gt;% select(fips, county, state = state_name, ru_ur_n = rucc_2013, ru_ur_label = desc) results_2020_rural_urban \u0026lt;- results_2020 %\u0026gt;% inner_join(rural_urban_tidy %\u0026gt;% select(-state), by = \u0026#39;fips\u0026#39;)    Tidy and combine We now have two datasets to work with:\nOur dataset containing the voting outcome Our dataset with the rural-urban codes  We’re going to transform our voting data to tidy format so we can join it with the rural-urban data. During this process we’ll also make some adjustments to what will become our axis labels. Adding two asterisks (**) will signal the ggtext package to bold certian words in the ggplot.\nru_ur_partison_vote \u0026lt;- results_2020_rural_urban %\u0026gt;% select(ru_ur_label, ru_ur_n, results_bidenj, results_trumpd) %\u0026gt;% gather(key, partison_vote, -ru_ur_label, -ru_ur_n) %\u0026gt;% group_by(ru_ur_label) %\u0026gt;% mutate(total_ru_ur_vote = sum(partison_vote), key = str_remove_all(key, \u0026#39;results_\u0026#39;) ) %\u0026gt;% ungroup() ru_ur_new_v \u0026lt;- c(\u0026quot;**Urban** population of 2,500 to 19,999, not adjacent to a metro area\u0026quot;, \u0026quot;**Urban** population of 2,500 to 19,999, adjacent to a metro area\u0026quot;, \u0026quot;**Urban** population of 20,000 or more, not adjacent to a metro area\u0026quot;, \u0026quot;Counties in **metro** areas of 250,000 to 1 million population\u0026quot;, \u0026quot;Counties in **metro** areas of fewer than 250,000 population\u0026quot;, \u0026quot;Counties in **metro** areas of 1 million population or more\u0026quot;, \u0026quot;Completely **rural** or less than 2,500 urban population, adjacent to a metro area\u0026quot;, \u0026quot;Completely **rural** or less than 2,500 urban population, not adjacent to a metro area\u0026quot;, \u0026quot;**Urban** population of 20,000 or more, adjacent to a metro area\u0026quot;) %\u0026gt;% str_replace_all(\u0026#39;, not\u0026#39;, \u0026#39;\u0026lt;br\u0026gt; not\u0026#39;) %\u0026gt;% str_replace_all(\u0026#39;, adjacent\u0026#39;, \u0026#39;\u0026lt;br\u0026gt; adjacent\u0026#39;) ru_ur_partison_vote \u0026lt;- ru_ur_partison_vote %\u0026gt;% distinct(ru_ur_label) %\u0026gt;% mutate(ru_ur_label_new = ru_ur_new_v) %\u0026gt;% right_join(ru_ur_partison_vote) %\u0026gt;% select(ru_ur_label = ru_ur_label_new, ru_ur_n, key, partison_vote, total_ru_ur_vote) Sub-setting for annotations Adding annotations to a plot can really help drive home the main take-aways by drawing the reader’s eye to specific areas.\n A key finding with our analysis, related to rural voting patterns, is that many people in urban areas voted Donald Trump. And because the urban areas have such a higher percentage of people living there it is much more impactful on the election outcome than rural areas.\n We’re going to highlight those urban vote percentages and add them to geom_text in the ggplot function. We can specify a new data set in that geom (separate from the primary data that’s being plotted in the main call).\nbar_labels_df \u0026lt;- ru_ur_partison_vote %\u0026gt;% group_by(ru_ur_label, key) %\u0026gt;% summarise(tot_ru_ur_party = sum(partison_vote)) %\u0026gt;% mutate(percent_ru_ur_party = percent(tot_ru_ur_party/sum(tot_ru_ur_party)) ) %\u0026gt;% ungroup() %\u0026gt;% mutate(position_y = tot_ru_ur_party/2, position_done = ifelse( key == \u0026#39;bidenj\u0026#39;, position_y + lead(tot_ru_ur_party), position_y )) %\u0026gt;% group_by(ru_ur_label) %\u0026gt;% mutate(tot = sum(tot_ru_ur_party)) %\u0026gt;% ungroup() Our bar colours will split out by the proportion of a every level won by one of the two main candidates.\nI searched the web and found the Republican party red and Democratic party blue. You can see these colours below.\nshow_col(c( \u0026#39;#0015BC\u0026#39;, \u0026#39;#DE0100\u0026#39;)) We can use these unique colours in the scale_fill_manual function. Also, the label_number_si function from the scales package provides the clean looking axes labels.\n# the scale_fill_manual requires a named vector, with the names corresponding to the levels to be filled party_cols \u0026lt;- c( \u0026#39;#0015BC\u0026#39;, \u0026#39;#DE0100\u0026#39;) names(party_cols) \u0026lt;- unique(ru_ur_partison_vote$key) # for clean labels gg_labs \u0026lt;- label_number_si(accuracy = 0.01) gg_wrap \u0026lt;- function(width = 25){ str_wrap(x, width) }   Building the plot by layers ggplot2 works by layering geoms on top of one another. This works to our advantage, as we can step through each additional plot layer at a time.\nFirst, let’s create the base plot with data. Note the use of mutate and fct_reorder to ensure our plot level order corresponds to the highest to lowest. We’ll start with a simple bar plot.\n(ru_ur_partison_vote_gg \u0026lt;- ru_ur_partison_vote %\u0026gt;% mutate(ru_ur_label = fct_reorder( ru_ur_label, total_ru_ur_vote), prefix = word(ru_ur_label, 1, 1) ) %\u0026gt;% ggplot(aes(ru_ur_label, partison_vote, fill = key))+ geom_col(show.legend = F)) The plot above shows we’re on the right track, but for the most part it’s unreadable at the moment. Before flipping the coordinates, we’re going to add the annotations to the plot. These are contained in the bar_labels_df data.\n(ru_ur_partison_vote_gg \u0026lt;- ru_ur_partison_vote_gg + geom_text(data = bar_labels_df %\u0026gt;% filter(str_detect(ru_ur_label, \u0026#39;^Counties\u0026#39;)) , aes(x = ru_ur_label, y = position_done, label = percent_ru_ur_party), color = \u0026#39;white\u0026#39;)+ geom_text(data = bar_labels_df, aes(x = ru_ur_label, y = tot+1.5e5, label = comma(tot)), hjust = 0, color = \u0026#39;black\u0026#39;, size = 3 )) Now we add the wow factor: the colours and label functions. Also, the coord_flip call flips the chart on its side - were we can more easily see everything.\n(ru_ur_partison_vote_gg \u0026lt;- ru_ur_partison_vote_gg + scale_x_discrete(labels = function(x){str_wrap(x, 25)})+ scale_y_continuous(labels = gg_labs)+ scale_fill_manual(values = party_cols)+ coord_flip(clip = \u0026#39;off\u0026#39;)) And finally, we add the a custom theme and some labels. The ggtext package controls are specified in the plot theme. We use the element_markdown and the element_textbox_simple to turn on the HTML and Markdown elements in the label text.\n(ru_ur_partison_vote_gg \u0026lt;- ru_ur_partison_vote_gg + labs( title = \u0026quot;\u0026lt;b\u0026gt;**US 2020 Elections by Rural-Urban Classification**\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt; Comparing \u0026lt;span style = \u0026#39;color:#DE0100;font-size:24pt\u0026#39;\u0026gt;**Donald J. Trump**\u0026lt;/span\u0026gt; \u0026amp; \u0026lt;span style = \u0026#39;color:#0015BC;font-size:24pt\u0026#39;\u0026gt;**Joe R. Biden**\u0026lt;/span\u0026gt;\u0026quot;, x = \u0026#39;USDA Rural-Urban\\nContinuum 2013\u0026#39;, y = \u0026#39;Total Votes\u0026#39;, caption = \u0026#39;Original data are from New York Times API.\\nParsed by Fabio Votta (@favstats), and downloaded from https://github.com/favstats/USElection2020-NYT-Results.\\nCreated by Elliot Meador (@Elliot_Meador).\u0026#39;) + post_theme( axis.text.y = element_markdown(), plot.title.position = \u0026quot;plot\u0026quot;, plot.title = element_textbox_simple( size = 18, lineheight = 1, padding = margin(5.5, 5.5, 5.5, 5.5), margin = margin(0, 0, 5.5, 0), ), plot.margin = margin(2,2,2,2, \u0026#39;cm\u0026#39;))) We can revisit our initial research questions and see that within rural areas there really isn’t enough people there to swing a national election. Also, Trump won over 40% of the big metro areas in the US, and even higher in some urban areas. More research is needed, but I personally don’t buy the argument that Trumps’ supporters are contained in rural areas only whilst Biden’s are found in the urban centres. On the contrary, it a much more mixed bag for both parties.\n ","date":1604793600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604835284,"objectID":"cc689a74e6ed679156144dcde60a9356","permalink":"/post/us-2020-presidental-election-and-rural-urban-divide/","publishdate":"2020-11-08T00:00:00Z","relpermalink":"/post/us-2020-presidental-election-and-rural-urban-divide/","section":"post","summary":"Introduction The US Presidential vote count has nearly finished. Trump is fighting the results in court though appears to have lost, but a clear theme that’s coming out is the contrast of rural and urban voters. The theory seems to go: Trump voters for the most part come from rural areas, and the election is based on a rural vs urban debate. We can test this theory using some data science approaches - mainly, cleaning and merging data from several places into a useful format.","tags":["Data","ggplot","dplyr"],"title":"US 2020 Presidental Election and Rural - Urban Divide","type":"post"},{"authors":[],"categories":["R","Rural"],"content":"     Introduction A considerable issue today related to food and rural population research are food deserts. Food deserts are a complicated issue, but the idea centres on a simple premise: areas, where it’s hard to reach a grocery store or access food, can be thought of as food deserts. If you’re interested in knowing more about the discourse on food deserts, I’d recommend looking into these papers:\n Blanchard, T. C., \u0026amp; Matthews, T. L. (2007). Retail concentration, food deserts, and food-disadvantaged communities in rural America. Remaking the North American food system: Strategies for sustainability, 201-215. Gundersen, C., Kreider, B., \u0026amp; Pepper, J. V. (2017). Partial identification methods for evaluating food assistance programs: a case study of the causal impact of SNAP on food insecurity. American Journal of Agricultural Economics, 99(4), 875-893. Andrews, M., Bhatta, R., \u0026amp; Ploeg, M. V. (2013). An alternative to developing stores in food deserts: can changes in SNAP benefits make a difference?. Applied Economic Perspectives and Policy, 35(1), 150-170.   The data Data for this post is on food deserts and comes from Kaggle. Kaggle is an excellent resource for aspiring data scientists and experienced ones. Specifically, the page called Food Deserts in the US: Food access for sub-populations of the United States. You can download the data directly from the link in the sentence above. The code below reads in the data.\nLibraries and data # For data wrangling/tables library(tidyverse) library(janitor) library(knitr) library(kableExtra) library(glue) library(tidytext) library(scales) # For mapping and census data library(tigris) library(leaflet) library(leaflet.extras) library(maps) library(sf) library(widgetframe) library(htmlwidgets) library(htmltools) map \u0026lt;- purrr::map # US county and state fips data is built-in R data(\u0026quot;county.fips\u0026quot;) county.fips \u0026lt;- county.fips %\u0026gt;% as_tibble() data(\u0026quot;state.fips\u0026quot;) state.fips \u0026lt;- state.fips %\u0026gt;% as_tibble() # File 1 food_access_research_atlas \u0026lt;- read_csv(\u0026#39;~/Downloads/665808_1173338_bundle_archive/food_access_research_atlas.csv\u0026#39;) %\u0026gt;% clean_names() # File 2 lookup \u0026lt;- read_csv(\u0026#39;~/Downloads/665808_1173338_bundle_archive/food_access_variable_lookup.csv\u0026#39;) %\u0026gt;% clean_names() # Pull in the USDA data from a directory I created in my cloud storage. rural_urban \u0026lt;- read_csv(\u0026#39;~/OneDrive - SRUC/Data/usda/ruralurbancodes2013.csv\u0026#39;) %\u0026gt;% select(state, county = county_name, rucc_2013, desc = description)  Theme We’ll use a custom theme for ggplot2 plots made with this code. Note the ... notation, which allows us to make on-the-fly changes without calling another theme argument.\n# a custom theme for ggplots post_theme \u0026lt;- function(...) { theme( text = element_text( color = \u0026#39;black\u0026#39;, family = \u0026#39;serif\u0026#39;), axis.text = element_text( color = \u0026#39;black\u0026#39;, size = 12.5), panel.background = element_blank(), axis.line.x = element_line( color = \u0026#39;black\u0026#39;), axis.ticks = element_blank(), plot.margin = margin(.75, .75, .75, .75, \u0026#39;cm\u0026#39;), plot.caption = element_text(hjust = 0, face = \u0026quot;italic\u0026quot;), plot.title = element_text( face = \u0026#39;bold\u0026#39;), plot.subtitle = element_text(face = \u0026#39;bold\u0026#39;), plot.title.position = \u0026quot;plot\u0026quot;, plot.caption.position = \u0026quot;plot\u0026quot;, strip.background = element_blank(), strip.text = element_text( face = \u0026#39;bold\u0026#39;) ) + theme(...) # this bit allows us to make changes using this same function instead of calling two theme functions. } An exciting caveat with the data is that it comes in two files. One file is the data, and the other file is what’s known as a data dictionary or data lookup. The lookup file is a database that explains what each variable is.\n Linking with rural-urban classification data There isn’t a perfect indicator of rural/urban classification in the data, so, as usual, we’ll need to add one. I’ve used the USDA Rural-Urban Classifications before in the post on Covid-19 and Rural Areas in the U.S..\n# clean names will be used to help get joining data in format that will merge well clean_names \u0026lt;- function(x){ x \u0026lt;- str_to_lower(x) x \u0026lt;- str_remove_all(x, \u0026#39;county\u0026#39;) x \u0026lt;- str_remove_all(x, \u0026#39;[[:digit:]]\u0026#39;) x \u0026lt;- str_squish(x) x \u0026lt;- str_trim(x) x } rural_urban \u0026lt;- rural_urban %\u0026gt;% mutate(county = clean_names(county)) %\u0026gt;% rename(abb = state) %\u0026gt;% left_join( tibble(abb = state.abb, state = clean_names(state.name)), by = \u0026quot;abb\u0026quot;) %\u0026gt;% mutate(state_county = glue(\u0026#39;{state} {county}\u0026#39;)) %\u0026gt;% select(state_county, rucc_2013, desc) rural_food_desert \u0026lt;- food_access_research_atlas %\u0026gt;% mutate(state = clean_names(state), county = clean_names(county), state_county = glue(\u0026#39;{state} {county}\u0026#39;)) %\u0026gt;% left_join(rural_urban, by = \u0026#39;state_county\u0026#39;)   Working with lookup files - A closer look at SNAP Lookup files are often called data dictionaries. Data dictionaries are a common component of many large datasets that are used extensively in the public sphere. As previously mentioned, quite often, a data dictionary or lookup is just another file that accompanies the primary data.\nThe data dictionary isn’t “data” in the traditional sense (i.e. we aren’t going to be performing cross-tabulations on it anytime soon), but it is a useful approach to treat it just like any other data file. For instance:\nWe can append data dictionaries/lookups to the central database as we did previously; We can use natural language processing on the data dictionaries to get a better understanding of the data holds; and, We can use data tools like dplyr and purrr to pick apart the lookup file and make it more manageable.  Let’s say we are interested in looking at SNAP. SNAP is short for Supplemental Nutrition Assistance Program. According to the USDA’s website:\n SNAP provides nutrition benefits to supplement the food budget of needy families so they can purchase healthy food and move towards self-sufficiency.\n We can use dplyr and the kable function from knitr to quickly search and display the results for variables on SNAP.\nlookup %\u0026gt;% filter(str_detect(description, \u0026#39;SNAP\u0026#39;)) %\u0026gt;% mutate(long_name = str_trunc(long_name, 10)) %\u0026gt;% kable(format = \u0026#39;html\u0026#39;, booktab = T) %\u0026gt;% kable_styling()   field  long_name  description      lasnaphalf  Low acc…  Housing units receiving SNAP benefits count beyond 1/2 mile from supermarket    lasnaphalfshare  Low acc…  Share of tract housing units receiving SNAP benefits count beyond 1/2 mile from supermarket    lasnap1  Low acc…  Housing units receiving SNAP benefits count beyond 1 mile from supermarket    lasnap1share  Low acc…  Share of tract housing units receiving SNAP benefits count beyond 1 mile from supermarket    lasnap10  Low acc…  Housing units receiving SNAP benefits count beyond 10 miles from supermarket    lasnap10share  Low acc…  Share of tract housing units receiving SNAP benefits count beyond 10 miles from supermarket    lasnap20  Low acc…  Housing units receiving SNAP benefits count beyond 20 miles from supermarket    lasnap20share  Low acc…  Share of tract housing units receiving SNAP benefits count beyond 20 miles from supermarket    TractSNAP  Tract h…  Total count of housing units receiving SNAP benefits in tract     Glancing at the table above, one will quickly see nine variables cover SNAP benefits. Also, it appears that SNAP variables are often distinguished by have either number or share, which means that the variable has either total counts of percents of occurrence.\nLet’s take a look at some and compare them by state. We’ll use the verb contains within dyplyr to grab variables contain SNAP.\nrural_food_desert_snap \u0026lt;- rural_food_desert %\u0026gt;% select(state, county, desc, census_tract, median_family_income, contains(\u0026#39;snap\u0026#39;))   Getting things tidy Before going forward we’ll need to tidy our data a bit. Note the use of pivot_longer instead of spread; pivot_longer and pivot_wider are dplyr’s new verb names for changing between wide and long formats. We won’t discuss the major changes here, but it’s good practice to read over big changes like this; you can do so here. Once we have our data in a way that we like it let’s do some quick plots, first looking at state and county levels, then looking at rural-urban areas.\nTravel distance county_snap_dist \u0026lt;- rural_food_desert_snap %\u0026gt;% mutate(id = group_indices(., census_tract)) %\u0026gt;% select(state, county, desc, median_family_income, census_tract, contains(\u0026#39;share\u0026#39;)) %\u0026gt;% pivot_longer(-c(state, county, desc, census_tract, median_family_income), names_to = \u0026#39;snap_dist\u0026#39;, values_to = \u0026#39;rate\u0026#39;) %\u0026gt;% mutate(snap_dist = parse_number(snap_dist), snap_dist = replace_na(snap_dist, .5))  Let’s quickly take a look at the top rate (percent) of people in census tracts for each distance from a supermarket. In addition, we’ll consider the association between rate of SNAP recipients to income.\nThe plot below uses the stat_binhex in ggplot. This approach is similar to a standard scatter plot, but it shows which areas of the plot have the highest frequency. This is important as the plot will have 124,326 points - far too many for a person to visual see the difference (due to overlapping with the points). The stat_binhex function uses colour to differentiate the frequencies for each bin.\nlabel_new \u0026lt;- function(x){ glue(\u0026#39;{x} miles to\\nnearest supermarket\u0026#39;) } county_snap_dist %\u0026gt;% mutate(snap_dist = as.factor(snap_dist), snap_dist = fct_relabel(snap_dist, .fun = label_new)) %\u0026gt;% filter(rate \u0026gt; 0) %\u0026gt;% ggplot(aes(median_family_income, rate))+ stat_binhex()+ geom_smooth(color = \u0026#39;#CCCCCC\u0026#39;, method = \u0026#39;gam\u0026#39;)+ scale_fill_viridis_c(labels = comma, name = \u0026#39;# of\\ncensus tracts\u0026#39;)+ scale_x_continuous(labels = dollar)+ scale_y_continuous(labels = percent, expand = c(0, 0))+ facet_grid(~snap_dist, )+ post_theme(axis.text.x = element_text(angle = 45, hjust = 1))+ labs(title = \u0026#39;Association between SNAP rate and family income by travel distance to supermarket\u0026#39;, x = \u0026#39;Median family income\\nper census tract\u0026#39;, y = \u0026#39;Percent SNAP\\nrecipient\u0026#39;, caption = \u0026#39;Smoothed line fits a generalized additive model (GAM) to data: y ~ s(x, bs = \u0026quot;cs\u0026quot;).\u0026#39;) According to plot above, we can see that there is a relationship between median family income and percentage of SNAP usage. Interestingly, but ultimately out of scope for this project, is the high percentage of SNAP recievers in places with high median family income values ($100k or more). We see this because both SNAP and median family income are measurements of central tendency within a geography. They are not 1-to-1 comparisons. That is, we aren’t looking at survey data completed by individual people. This finding alone tells us that it is necessary to look deeper into SNAP, as families who recieve the benefit may live quite unique lives, each with their own struggles to overcome.\n  Building interactive maps Our data is aggregated by census tract and is therefore geographical by nature. Mapping in R has made considerable developments in the past 5 to 10 years, and any work with rural/urban analysis can usually be benefited through some geospatial analysis. So learning to make maps is always helpful!\nLeaflet maps The leaflet package is a fantastic way to make interactive maps with a relatively small amount of code. It works really well will with the sf package for geo-computational analysis. Moreover, we can use the tigris package to get census tract information straight in R. tigris can return sf objects, making for speedy workflow between the three packages.\nBelow, we’ll take a look those census tracts that have a large proportion of the population that have to travel 20 or miles to a supermarket and that have a high percentage of SNAP recipients.\n## Get all 20 miles or more that have at least some percent of SNAP users top_perc_20m \u0026lt;- county_snap_dist %\u0026gt;% filter(snap_dist == 20, rate \u0026gt; 0) %\u0026gt;% mutate(desc = replace_na(desc, \u0026#39;Unknown\u0026#39;)) top_perc_20m_ls \u0026lt;- top_perc_20m %\u0026gt;% mutate(id = as.character(row_number())) %\u0026gt;% split(.$id) # retrieve all census tract polygons per county using a loop and the applying the tracts function from the tigris package. top_perc_20_sf_ls \u0026lt;- map(top_perc_20m_ls, possibly(function(x){ .y \u0026lt;- tracts(state = x$state, county = x$county, cb = T) single_track \u0026lt;- str_sub(x$census_tract, start = 6, end = 9) .z \u0026lt;- .y %\u0026gt;% filter(NAME %in% single_track) .z$census_tract \u0026lt;- x$census_tract .z$rate \u0026lt;- x$rate .z$dist \u0026lt;- x$snap_dist .z$desc \u0026lt;- x$desc .z$id \u0026lt;- x$id .z$county \u0026lt;- x$county .z$state \u0026lt;- x$state .z$median_family_income \u0026lt;-x$median_family_income return(.z) }, NULL)) # NOTE do.call to combine the sf objects top_20_sf \u0026lt;- do.call(rbind,top_perc_20_sf_ls) # This whole code-block may take quite a bit of time to run depending on your computer\u0026#39;s specs. It\u0026#39;s best to go ahead and save the output and then comment out the code above. This reduces the risk of accidently changing or re-running things, then having to wait to make adjustments. write_sf(top_20_sf,\u0026#39;~/Documents/temp/top_20_sf.shp\u0026#39;) It only takes a few lines of code to produce a great map in leaflet. Below you can see the polygons of the top 25 SNAP recipients in food deserts that are 20 miles or more to supermarkets. And, while it is inciteful in its own right, it leaves us wanting something more. With the help of HTML, we can turn this map into something fantastic!\nleaflet(top_20_sf) %\u0026gt;% addProviderTiles(\u0026quot;CartoDB.Positron\u0026quot;) %\u0026gt;% addPolygons(color = \u0026quot;tomato\u0026quot;) %\u0026gt;% frameWidget()  {\"x\":{\"url\":\"/post/2020-07-29-u-s-food-deserts_files/figure-html//widgets/widget_unnamed-chunk-11.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]} Leaflet to the next level One way to dramatically improve our interactive leaflet maps is to use HTML code. HTML is essential a coding approach to formatting web applications. Officially HTML is:\n Hypertext Markup Language (HTML) is the standard markup language for documents designed to be displayed in a web browser. It can be assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript.\n We’ll also use CSS, which is similar to HTML in that it’s used to format websites. Officially, CSS is:\n CSS stands for Cascading Style Sheets. CSS describes how HTML elements are to be displayed on screen, paper, or in other media. CSS saves a lot of work. It can control the layout of multiple web pages all at once. External stylesheets are stored in CSS files.\n We can also use our data to correspond to map polygon colours. The leaflet.extras package offers a lot of great extra options to add to leaflet maps. The best part is that it is relatively straightforward to add these options.\nLet’s take the following steps to trick out our leaflet map!\nWe’ll use HTML to create some tooltips that provide users information when they hover over polygons in the map. Create a custom function to map the fill/colour of the leaflet polygons to help guide the user’s eye towards the worst off places in terms of SNAP and distance to supermarket. NOTE: Our custom colour function is from this question on StackOverflow.com. Use leaflet.extras to add some great functionality that makes the map more user-friendly. Create and format a title using CSS.  remove_words \u0026lt;- glue_collapse(c(\u0026#39;Nonmetro - \u0026#39;, \u0026#39;Metro - \u0026#39;),\u0026#39;|\u0026#39;) ## tooltip with html tooltip \u0026lt;- top_20_sf %\u0026gt;% as_tibble() %\u0026gt;% mutate(county = str_to_title(county), state = str_to_title(state), rate = round(rate, 2), rate = percent(rate), desc = str_remove_all(desc, remove_words), median_family_income = dollar(median_family_income)) %\u0026gt;% transmute(tip = glue(\u0026#39;\u0026lt;b\u0026gt;County:\u0026lt;/b\u0026gt; {county} \u0026lt;br\u0026gt; \u0026lt;b\u0026gt;State:\u0026lt;/b\u0026gt; {state} \u0026lt;br\u0026gt; \u0026lt;b\u0026gt;*SNAP percent:\u0026lt;/b\u0026gt; {rate} \u0026lt;br\u0026gt; \u0026lt;b\u0026gt;**Rural/Urban class:\u0026lt;/b\u0026gt; {desc} \u0026lt;br\u0026gt; \u0026lt;b\u0026gt;Median family income:\u0026lt;/b\u0026gt; {median_family_income} \u0026lt;br\u0026gt;\u0026lt;b\u0026gt;Tract:\u0026lt;/b\u0026gt; {census_tract}\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;* All areas have this percentage SNAP recipients who 20 or more miles from a supermarket.\u0026lt;br\u0026gt;** Rural/Urban classification determined at county-level.\u0026#39;)) # change polygon colour to correspond to a numeric variable in the database map2color \u0026lt;- function(x, pal, limits = NULL) { if (is.null(limits)) limits = range(x) pal[findInterval(x, seq(limits[1], limits[2], length.out = length(pal) + 1), all.inside = TRUE)] } col_pal \u0026lt;- rev(viridis_pal()(6)) all_rate \u0026lt;- top_20_sf %\u0026gt;% as_tibble() %\u0026gt;% pull(rate) all_rate_0 \u0026lt;- ifelse(all_rate \u0026lt; 0.005, 0, all_rate) all_rate_6 \u0026lt;- cut_interval(all_rate_0, 6, labels = F) percent_labs \u0026lt;- c(\u0026#39;0.0% to 10.1%\u0026#39;, \u0026#39;10.1% to 20.2%\u0026#39;, \u0026#39;20.2% to 30.2%\u0026#39;, \u0026#39;30.2% to 40.3%\u0026#39;, \u0026#39;40.3% to 50.4%\u0026#39;, \u0026#39;50.4% to 60.5%\u0026#39;) ## Add a our title to the map using css tag.map.title \u0026lt;- tags$style(HTML(\u0026quot; .leaflet-control.map-title { transform: translate(-50%,20%); position: fixed !important; left: 50%; text-align: center; padding-left: 10px; padding-right: 10px; background: rgba(255,255,255,0.5); font-weight: bold; font-size: 28px; } \u0026quot;)) title \u0026lt;- tags$div( tag.map.title, HTML(\u0026quot;US Food Deserts\u0026quot;) ) top_20_sf_leaf_map \u0026lt;- leaflet(top_20_sf) %\u0026gt;% addProviderTiles(\u0026quot;OpenStreetMap\u0026quot;) %\u0026gt;% addPolygons(color = \u0026#39;#C4C4C4\u0026#39;, fillColor = map2color(all_rate_6, col_pal), fillOpacity = .85, weight = .55, popup = tooltip$tip, opacity = 1 )%\u0026gt;% addDrawToolbar( editOptions=editToolbarOptions(selectedPathOptions=selectedPathOptions()) ) %\u0026gt;% addLegend(colors = col_pal, labels = percent_labs, opacity = 0.7, title = \u0026#39;Percent SNAP use\u0026lt;br\u0026gt;by census tract\u0026#39;, position = \u0026quot;bottomright\u0026quot;) %\u0026gt;% addControl(title, position = \u0026quot;topleft\u0026quot;, className=\u0026quot;map-title\u0026quot;) top_20_sf_leaf_map %\u0026gt;% frameWidget(width = \u0026#39;100%\u0026#39;)  {\"x\":{\"url\":\"/post/2020-07-29-u-s-food-deserts_files/figure-html//widgets/widget_unnamed-chunk-12.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]} Our leaflet map now has plenty of bells and whistles, and hopefully, it will be useful for people interested in knowing more about food deserts and SNAP in the US.\n   Conclusion Food deserts and SNAP are complicated subjects in their own right. Very often, the two compound one another, making things even more complicated. Using data science approaches to thinking about these two issues can help policymakers and rural stakeholders better plan for those people impacted by them.\nAs always, the Deltanomics blog is for instructional use in R. Any potential findings need more research to verify them before any conclusions can be made.\n ","date":1597536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597615893,"objectID":"f227d16582dfb9d594fdaba26bfe1240","permalink":"/post/food-deserts-data-clean-merge/","publishdate":"2020-08-16T00:00:00Z","relpermalink":"/post/food-deserts-data-clean-merge/","section":"post","summary":"Introduction A considerable issue today related to food and rural population research are food deserts. Food deserts are a complicated issue, but the idea centres on a simple premise: areas, where it’s hard to reach a grocery store or access food, can be thought of as food deserts. If you’re interested in knowing more about the discourse on food deserts, I’d recommend looking into these papers:","tags":["Data","dplyr","ggplot"],"title":"U.S. Food Deserts","type":"post"},{"authors":["Elliot Meador PhD"],"categories":null,"content":"","date":1596844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596844800,"objectID":"fe4d6b0b025ded50b4606fff8c4dc184","permalink":"/talk/isna/","publishdate":"2020-08-08T00:00:00Z","relpermalink":"/talk/isna/","section":"talk","summary":"International Network for Social Network Analysis. Spoke on results from the TGRAINS project. This talk occured virtually due to Covid-19 restrictions on travel.","tags":null,"title":"Supporting sustainable and resilient food systems","type":"talk"},{"authors":[],"categories":["R","SNA"],"content":" Introduction I’ve been doing some work lately on social networks that exist between organisations or institutions. This is nice as it builds on some of my dissertation work, and I generally find it quite interesting. Networks that form between organisations are often quite powerful, in that they can illustrate where strong areas of like-minded work exist or where new connections might be useful in strengthing one organisation’s influence.\nWhy like me? Organisational ties – like personal relationships in business – can be extremely valuable to a firm’s bottom line. These ties are often leveraged and manoeuvred to produce some sought-after result. A few examples might be:\n Do I know anyone at Organisation X who might have more information about some big grant schemed about to be launched?\n  Who in my inner circle has ties to a business who might provide some venture capital for an idea I have?\n The notion of leverage’s one’s networks is nothing new. It’s known as social capital theory in sociology and is a well-known tactic taught in business schools. Its applications are too big to get into in this blog post. I’d suggest taking a look at this Forbes article for a bit more information on why this might benefit you. If you’re interested in leveraging one’s ties for their benefit from an academic perspective you can read some of my published articles or read works by James Coleman, Pierre Bourdeau or Robert Putnam from the 1980s and 1990s.\n  The nitty-gritty Some data Now that we know it’s a worthwhile venture to leverage one’ network ties, let’s take a closer look at how we might approach this from a data perspective. We’ll use a portion of the mock data we created in an earlier blog found here. The database created from the code in that blog post is called create_sna_data, and we’ll be using it for the rest of this post.\n Finding what we’re looking for We’re going to be preforming shortest-distance analysis. Shortest distance analysis, also known as shortest paths, is a common algorithm used anytime objects are found within a network. It’s common in logistics work, especially regarding travel along a road or rail network. GIS analysis uses this approach as well. A great example is the Scottish Index of Multiple Deprivation’s use of shortest-distance analysis in looking at access as a form of deprivation in Scotland. Take a look at the SIMD documentation here for an overview.\nAccording to igraph’s documentation:\n … [shortest distance] calculates the length of all the shortest paths from or to the vertices in the network. [It] calculates one shortest path (the path itself, and not just its length) from or to the given vertex.\n Let’s use the shortest distance function from tidygraph (which implements igraph’s shortest.paths functionality) using a straight forward approach.\nBelow let’s look at the shortest distance between two nodes.\nFirst, we’ll create a base layout from which to filter for sub-graphs when we want to show a path between two nodes. Then, we’ll plot the graph using ggraph. ggraph likes the layout matrices to be in tibble form with the column names x and y.  # create a filtering column called id. create_sna_data \u0026lt;- create_sna_data %\u0026gt;% mutate(id. = row_number()) # use the layout with stress algorythm to create a layout layout_df \u0026lt;- create_sna_data %\u0026gt;% layout_with_stress() %\u0026gt;% as_tibble() %\u0026gt;% set_names(c(\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;) ) ## And now the plot create_sna_data %\u0026gt;% ggraph(layout = layout_df)+ # our layout algorythm from above geom_edge_fan(start_cap = circle(2, \u0026#39;mm\u0026#39;), end_cap = circle(2, \u0026#39;mm\u0026#39;))+ geom_node_point(size = 4)+ geom_node_text(aes(label = id.), size = 2.25, color = \u0026#39;white\u0026#39;)+ labs(title = \u0026#39;Full network\u0026#39;) Now, we’ll find the shortest path between two nodes: 75 and 4. We’ll plot the shortest path as a network and lay it on top of the network above so we can visualise the path within the network. To do this, we’ll calculate the layout tibble first and filter it for the shortest paths network using the layout_with_stressfunction from the graphlayouts package. Note that we’ll need to pull our nodes that sit along the shortest path from our layout data frame. We’ll use the slice function in dplyr for this.\ncreate_sna_data_4_75 \u0026lt;- create_sna_data %\u0026gt;% morph(to_shortest_path, 75, 4) %\u0026gt;% mutate(selected_node = T) %\u0026gt;% activate(edges) %\u0026gt;% mutate(selected_edge = T) %\u0026gt;% activate(nodes) %\u0026gt;% unmorph() colors_v \u0026lt;- c(\u0026#39;tomato\u0026#39;, \u0026#39;skyblue\u0026#39;) names(colors_v) \u0026lt;- c(\u0026#39;TRUE\u0026#39;, \u0026#39;Other\u0026#39;) create_sna_data_4_75 %\u0026gt;% mutate(selected_node = ifelse( is.na(selected_node), \u0026#39;Other\u0026#39;, selected_node )) %\u0026gt;% activate(edges) %\u0026gt;% mutate(selected_edge = ifelse( is.na(selected_edge), \u0026#39;Other\u0026#39;, selected_edge )) %\u0026gt;% ggraph(layout = layout_df)+ geom_edge_fan(aes(color = selected_edge), start_cap = circle(2, \u0026#39;mm\u0026#39;), end_cap = circle(2, \u0026#39;mm\u0026#39;))+ geom_node_point(aes(color = selected_node), size = 4)+ geom_node_text(aes(label = id.), size = 2.5, color = \u0026#39;white\u0026#39;)+ scale_color_manual(values = colors_v, name = \u0026#39;Nodes\u0026#39;, label = c(\u0026#39;Other\u0026#39;, \u0026#39;Shortest path\u0026#39;))+ scale_edge_color_manual(values = colors_v, name = \u0026#39;Edges\u0026#39;, label = c(\u0026#39;Other\u0026#39;, \u0026#39;Shortest path\u0026#39;))+ labs(title = \u0026#39;Shortest path between nodes 75 and 4\u0026#39;)  Leveraging networks The notion of leveraging networks comes from the reality that not every node in a network is available to us to draw resources from. For the most part, a node has a useable relationship with only those nodes in its inner circle – or its one-degree neighbourhood. Now, this isn’t always the case, but as a very general rule with SNA, the further one node is from another, the less influence they have on one another. SNA can help identify which people could put you in touch with some other person based on a set of pre-defined criteria. SNA helps answer the question:\n Do I know a dude who knows a dude?\n The goal So let’s suppose our goal is to utilise our social network to find a potential partner to work on a grant with us. The funder of our grant scheme has a real soft spot for loners – i.e. one-degree nodes; maybe because before our funder made it big they used to be a one-degree node themselves. Who knows! Below we see our same network with the one-degree nodes highlighted. We’ll pretend that we are Le, Brianna (# 87), one of the most well-connected persons in the network (with a betweenness score of 3,088).\nWe know that we want to partner with a node that has only-degree. We will want to approach as many one-degree nodes as possible, as some will turn us down or might not be available to partner on the grant application.\nWe also know that we get along with some people better than others and that we’ll have to depend on our relationships to help leverage them. It may sound crazy, but Le, Brianna (ourself) gets along really well with people who buy all their food from farmers markets. So, we’ll use this to our advantage by trying to find as many shortest paths to single-degree nodes that are filled with nodes that buy all their meals at farmers markets. We’ll do this with iterative programming otherwise known as looping. Therefore, our approach is:\nAssign a numeric scoring value for the column buy_farm_mark, with more meals receiving a higher score; geodesic distance will also be factored into the score, with nodes further away receiving a higher score. Identify all shortest paths between Le, Brianna and all one-degree nodes in the network. Create a scaled score for each path that we can use to decide on who to contact first for partnering on the grant application.  Step 1 # Those who buy every meal from the farmers # market get a score of 5, 3 for most meals # and 0 for hardly any meals. We\u0026#39;ll create # a tibble and merge it with data frame. farm_buy_n \u0026lt;- tibble(buy_farm_mark = c(\u0026#39;Every meal\u0026#39;, \u0026#39;Most meals\u0026#39;, \u0026#39;Hardly any meals\u0026#39;), farm_mark_score = c(5, 3, 0)) create_sna_data_updated \u0026lt;- create_sna_data_updated %\u0026gt;% left_join(farm_buy_n, by = \u0026quot;buy_farm_mark\u0026quot;)  Step 2 # get all one-degree node one_degree_names \u0026lt;- names( which( degree( create_sna_data_updated) == 1)) # and pull out their names. one_degree_ids \u0026lt;- which( V(create_sna_data_updated)$name %in% one_degree_names) # Find the max betweeness for the starting node. max_degree \u0026lt;- which( betweenness( create_sna_data_updated) == max( betweenness( create_sna_data_updated)))[1] ## The loop!! ## all_shortest_one_degre_paths_ls \u0026lt;- map(one_degree_ids, # our one-degree nodes are here function(x){ create_sna_data_updated %\u0026gt;% morph(to_shortest_path, max_degree, x) %\u0026gt;% mutate(selected_node = TRUE) %\u0026gt;% activate(edges) %\u0026gt;% mutate(selected_edge = TRUE) %\u0026gt;% activate(nodes) %\u0026gt;% unmorph() })  Step 3 # Create the scores and flatten the list # into a numeric vector that we can use to # subset by. all_scores \u0026lt;- all_shortest_one_degre_paths_ls %\u0026gt;% map(function(x){ x %\u0026gt;% filter(selected_node) %\u0026gt;% as_tibble() %\u0026gt;% summarise(total_farm = sum(farm_mark_score), n = n(), total_score = total_farm / n) %\u0026gt;% pull(total_score) }) %\u0026gt;% flatten_dbl() highest_score \u0026lt;- which(all_scores == max(all_scores))[[1]] And now let’s take a look at the final results.\n  Results Let’s plot our final results using the code below.\ncolor_v_iii \u0026lt;- c(\u0026#39;#084081\u0026#39;, \u0026#39;#A8DDB5\u0026#39;) names(color_v_iii) \u0026lt;- c(T, \u0026#39;Other\u0026#39;) highest_score_g \u0026lt;- all_shortest_one_degre_paths_ls[[ highest_score]] %\u0026gt;% mutate(selected_node = ifelse(is.na(selected_node), \u0026#39;Other\u0026#39;, selected_node)) %\u0026gt;% activate(edges) %\u0026gt;% mutate(selected_edge = ifelse(is.na(selected_edge), \u0026#39;Other\u0026#39;, selected_edge)) highest_score_g %\u0026gt;% activate(nodes) %\u0026gt;% mutate(buy_farm_mark = factor(buy_farm_mark, levels = c(\u0026#39;Every meal\u0026#39;, \u0026#39;Most meals\u0026#39;, \u0026#39;Hardly any meals\u0026#39;))) %\u0026gt;% ggraph(layout = layout_df)+ geom_edge_fan(aes(color = selected_edge))+ geom_node_point(aes(color = selected_node, shape = buy_farm_mark), size = 3)+ scale_color_manual(values = color_v_iii, \u0026#39;Node path\u0026#39;, labels = c(\u0026#39;Other\u0026#39;, \u0026#39;Highest score\u0026#39; ))+ scale_edge_color_manual(values = color_v_iii, \u0026#39;Edge path\u0026#39;, labels = c(\u0026#39;Other\u0026#39;, \u0026#39;Highest score\u0026#39; ))+ scale_shape(\u0026#39;Meals bought from\\nfarmers market\u0026#39;)+ labs(title = \u0026#39;Highest scoring-paths graph\u0026#39;) We can see that our approach to identifying the most appropriate project partner favours those nodes that eat every meal with food bought from the farmers market. Of course, this is just a demonstration of how looping can be used with network analysis to find optimum routes within a network.\n   ","date":1590883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590928321,"objectID":"59e04862b487efd69e3f89ab53d71304","permalink":"/post/sna/","publishdate":"2020-05-31T00:00:00Z","relpermalink":"/post/sna/","section":"post","summary":"Introduction I’ve been doing some work lately on social networks that exist between organisations or institutions. This is nice as it builds on some of my dissertation work, and I generally find it quite interesting. Networks that form between organisations are often quite powerful, in that they can illustrate where strong areas of like-minded work exist or where new connections might be useful in strengthing one organisation’s influence.\nWhy like me?","tags":["ggraph","tidygraph","network"],"title":"Be like me - looping through shortest distance analysis","type":"post"},{"authors":[],"categories":["R"],"content":" First published on 26-April-2020  Last updated on 16-Aug-2020  Introduction Now that we are square in the middle of the Covid-19 pandemic, I thought it might be beneficial to look at some statistics associated with the number of cases. We’ll differentiate our analysis by focusing on cases of Covid-19 in rural areas of the U.S. There are a couple of reasons for this: mainly, rural analytics is my speciality, so while I don’t know much about the virus, I do know some about rural societies and economies; we can easily find pertinent data on rural counties; and, we can utilise some cool built-in R functions to help us along the way. Before we go on it’s important to note that:\nI am not a medical doctor or specialist in viral diseases. This post is meant to be a learning resource for people interested in looking at the pandemic from a rural perspective. Any potential interesting findings must be further investigated before any judgements can be made.   Data The data for this post comes from two places: Covid-19 cases from the New York Times github; and the USDA Rural-Urban Classification Codes.\nData scientists at The New York Times have been collating data on the number of cases of Covid-19 by county in the U.S.. It is available at their GitHub page, which means that one can easily access and update the data through pull requests. The data can also be downloaded and saved to a local drive.\nTo get a rural understanding of Covid-19 cases, we’ll use the USDA data on rural-urban classification of U.S. counties, which can be downloaded using the hyperlink above. Many countries have geographical classifications for rural and urban spaces. Usually, a low-level geography is chosen that spans an entire country. A continuum of rural-urban is used to describe each geographical area that goes from very urban to very rural (though not using those specific labels).\nSo we’ll use those two datasets, join them together and investigate how many cases of Covid-19 are found in rural areas within the U.S..\n Analysis Libraries and themes Tidyverse packages will be used to do most of the heavy lifting. We’ll do the data analysis using the dplyr package, and we’ll do our plots with ggplot2.\nlibrary(tidyverse) library(lubridate) library(janitor) library(scales) library(cowplot) # USDA Rural-Urban classification codes post_theme \u0026lt;- function(...) { theme( text = element_text( color = \u0026#39;black\u0026#39;, family = \u0026#39;serif\u0026#39;), axis.text = element_text( color = \u0026#39;black\u0026#39;), axis.text.x = element_text(angle = 45, hjust = 1), panel.background = element_blank(), axis.line.x = element_line( color = \u0026#39;black\u0026#39;), axis.ticks = element_blank(), plot.margin = margin(.75, .75, .75, .75, \u0026#39;cm\u0026#39;), plot.caption = element_text(hjust = 0, face = \u0026quot;italic\u0026quot;), plot.title = element_text( face = \u0026#39;bold\u0026#39;), plot.subtitle = element_text(face = \u0026#39;bold\u0026#39;), plot.title.position = \u0026quot;plot\u0026quot;, plot.caption.position = \u0026quot;plot\u0026quot;, strip.background = element_blank(), strip.text = element_text( face = \u0026#39;bold\u0026#39;) ) + theme(...) # this bit allows us to make changes using this same function instead of calling two theme functions. } # I created a new R-project to house the Covid-19 data in my Documents directory. covid_county \u0026lt;- read_csv(\u0026#39;~/Documents/R/covid-19-data/us-counties.csv\u0026#39;) # Pull in the USDA data from a directory I created in my cloud storage. rural_urban \u0026lt;- read_csv(\u0026#39;~/OneDrive - SRUC/Data/usda/ruralurbancodes2013.csv\u0026#39;) %\u0026gt;% select(fips, rucc_2013, desc = description) Now that we’ve got the data loaded from Covid-19 and USDA Rural-Urban Classifications, we are going to use some of R’s base functionality. R has two functions that help with general data analysis and joins: state.name, which has all 50 U.S. state names; and, state.region, which has the 50 U.S. state’s categorised into geographical regions.\nWe’ll use these two functions in a tibble to help join the Covid-19 data with the Rural-Urban Classifications.\nstate_region \u0026lt;- tibble(state = state.name, region = state.region) covid_region \u0026lt;- covid_county %\u0026gt;% left_join(rural_urban, by = \u0026#39;fips\u0026#39;) %\u0026gt;% left_join(state_region, by = \u0026#39;state\u0026#39;) %\u0026gt;% mutate(week = floor_date(date, \u0026#39;week\u0026#39;)) Now we have our working data frame called covid_region. It has the following variable names: date, county, state, fips, cases, deaths, rucc_2013, desc, region, week. We’ll use the description variable to filter out rural counties only. There are two classifications of rural areas - those that are adjacent to more metro places and those that are not. Those that are not adjacent to metro areas are adjacent to other rural areas, which makes them somewhat more remote, as people living there need to travel further to get to service centres.\nWe use the group_by/summarise functionality from dplyr to find the sum of Covid-19 cases and deaths by each week, region and for both rural classifications.\nweekly_regions \u0026lt;- covid_region %\u0026gt;% filter(str_detect(desc, \u0026#39;rural\u0026#39;)) %\u0026gt;% group_by(week, region, desc) %\u0026gt;% summarise(cases = sum(cases, na.rm = T), deaths = sum(deaths, na.rm = T)) %\u0026gt;% ungroup() %\u0026gt;% gather(key, value,-c(week, region, desc)) %\u0026gt;% drop_na() We’re going to make a fancy-looking plot for this post, something that you might like to share on social media or include in a work report. To help clean up the plot a bit, we’ll use a few approaches that are laid out in the code below.\n# each label for the x-axis which we\u0026#39;ll use to make some nice looking data labels. week_n \u0026lt;- weekly_regions %\u0026gt;% count(week) %\u0026gt;% pull(week) ##------ Wed Jul 29 21:41:05 2020 ------## # added this to clean up the x-axis month_n \u0026lt;- week_n %\u0026gt;% floor_date(\u0026#39;month\u0026#39;) %\u0026gt;% unique() # we\u0026#39;ll use scale_color_manual with our own color choice col_v \u0026lt;- c(\u0026#39;#3E4A89FF\u0026#39;, \u0026#39;#FDE725FF\u0026#39;) names(col_v) \u0026lt;- unique(weekly_regions$desc) # a simple label_wrap function for the legend label_wrap \u0026lt;- function(x, n = 25){ paste0(str_wrap(x, n), \u0026#39;\\n\u0026#39;) } # date labels that use drops today\u0026#39;s date into the caption of the plot. today_date \u0026lt;- as_date(Sys.time()) %\u0026gt;% format(\u0026#39;%d %B, %Y\u0026#39;) OK, now we’ll create the main ggplot that uses facet_wrap to look at each region in the U.S. over time.\nweekly_regional_gg \u0026lt;- weekly_regions %\u0026gt;% filter(key == \u0026#39;cases\u0026#39;) %\u0026gt;% ggplot(aes(week, value, group = desc)) + geom_line(size = 1.25, aes(color = desc))+ geom_point(size = 4, color = \u0026#39;grey90\u0026#39;)+ geom_point(size = 3.5, aes(color = desc))+ scale_x_date(breaks = month_n, date_labels = \u0026#39;%d-%b\u0026#39;)+ scale_y_continuous(labels = comma) + scale_color_manual( values = col_v, labels = label_wrap, name = \u0026#39;Rural classification\u0026#39;)+ facet_wrap( ~ region) + post_theme()+ labs(caption = \u0026#39;Severe drop-offs may indicate that data was most recently updated earlier in the week.\u0026#39;) And now we’ll add the labels and annotations.\n(weekly_regional_gg \u0026lt;- weekly_regional_gg + labs( title = \u0026#39;Weekly Total U.S. COVID-19 Cases by Region in Rural Counties\u0026#39;, subtitle = \u0026#39;SOURCE: The New York Times, based on reports from state and local health agencies \u0026amp;\\nThe USDA Rural-Urban Continuum Codes (2013).\u0026#39;, x = \u0026#39;\\nWeek of\u0026#39;, y = \u0026#39;Total\u0026#39;, color = str_wrap(\u0026#39;USDA Rural-Urban Continuum Codes (2013)\u0026#39;, 25), caption = str_c(\u0026#39;By Elliot Meador, PhD; @Elliot_Meador\\nNOTE: Last week may not yet be complete.\\nProduced \u0026#39;, today_date))) It looks as though rural counties in the south are reporting more cases than the rest of the U.S.. It’s worth investigating the southern counties to see if one state/county is pulling the statistics higher for the entire region, or if the trend is true for the majority of counties. There are a few ways to do this, but the most straightforward is to replicate the plot above but for southern states only. The code below does this.\nsouthern_rural_cases \u0026lt;- covid_region %\u0026gt;% filter(str_detect(desc, \u0026#39;rural\u0026#39;), region == \u0026#39;South\u0026#39;) %\u0026gt;% group_by(week, state, desc) %\u0026gt;% summarise(total = sum(cases, na.rm = T)) %\u0026gt;% ungroup() viridis \u0026lt;- scales::viridis_pal() southern_state \u0026lt;- southern_rural_cases %\u0026gt;% count(state) %\u0026gt;% pull(state) state_cols \u0026lt;- viridis(length(southern_state)) names(state_cols) \u0026lt;- southern_state labels_df \u0026lt;- southern_rural_cases %\u0026gt;% group_by(state, desc) %\u0026gt;% filter(week == max(week)) %\u0026gt;% ungroup() %\u0026gt;% mutate(desc = str_wrap(desc, 35)) weeks_lab \u0026lt;- southern_rural_cases %\u0026gt;% count(week) %\u0026gt;% pull(week) southern_plot \u0026lt;- southern_rural_cases %\u0026gt;% mutate(desc = str_wrap(desc, 35)) %\u0026gt;% ggplot(aes(week, total, group = state))+ geom_line(aes(color = state), show.legend = F)+ geom_point(color = \u0026#39;white\u0026#39;, size = 2.25)+ geom_point(aes(color = state), size = 2, show.legend = F)+ geom_text(data = labels_df, aes(label = state, x = week, y = total, color = state), size = 2, hjust = 0, nudge_x = 1.25, check_overlap = T, show.legend = F)+ scale_x_date(breaks = weeks_lab, date_labels = \u0026#39;%b-%d\u0026#39;)+ scale_color_manual(values = state_cols)+ scale_y_log10(labels = comma)+ facet_grid(~desc)+ coord_cartesian(clip = \u0026#39;off\u0026#39;)+ post_theme(plot.margin = margin(1.25, 1.25, 1.25, 1.25, \u0026#39;cm\u0026#39;), panel.spacing = unit(2, \u0026quot;lines\u0026quot;)) And just like above, we’ll add our labels seperate.\n(southern_plot_ii \u0026lt;- southern_plot+ labs(title = \u0026#39;Comparing Covid-19 Cases Across Rural Counties in the Southern U.S.\u0026#39;, subtitle = \u0026#39;SOURCE: The New York Times, based on reports from state and local health agencies \u0026amp;\\nThe USDA Rural-Urban Continuum Codes (2013).\u0026#39;, x = \u0026#39;Week of\u0026#39;, y = \u0026#39;Total\\nLog10-scale\u0026#39;, caption = str_c(\u0026#39;By Elliot Meador, PhD; @Elliot_Meador\\nNOTE: Last week may not yet be complete.\\nProduced \u0026#39;, today_date)))   County-level analysis In the above analysis, we are showing aggregate statistics across states. This gives a good overall understanding of high-level trends, but the next step is to look a bit closer at what happens at a more granular level. Let’s take a look at all rural counties in the U.S. and plot the total cases by the total deaths - which is a common plot I’ve found online.\nWe’ll only look at rural counties that have at least 10 recorded cases. We’re going to do a twist on a standard scatterplot, where we plot the state abbreviation of the county instead of a simple point. We’ll also colour all abbreviations of the same state in the same colour; this will help draw the readers’ eye to similar states. Lastly, we won’t have a colour legend as this many states will lead to a massive legend that will overpower the plot.\ncovid_county_rural \u0026lt;- covid_county %\u0026gt;% left_join(rural_urban, by = \u0026#39;fips\u0026#39;) %\u0026gt;% filter(str_detect(desc, \u0026#39;rural\u0026#39;)) %\u0026gt;% select(-date, -county, -rucc_2013) %\u0026gt;% group_by(fips) %\u0026gt;% mutate(tot_deaths = sum(deaths, na.rm = T), tot_cases = sum(cases, na.rm = T)) %\u0026gt;% ungroup() %\u0026gt;% filter(tot_cases \u0026gt; 10, tot_deaths \u0026gt; 1) %\u0026gt;% # must have at least 10 cases select(-cases, -deaths) %\u0026gt;% distinct(fips, .keep_all = T) plot_states \u0026lt;- covid_county_rural %\u0026gt;% distinct(state) %\u0026gt;% pull() state_cols \u0026lt;- sample(viridis_pal()(length(plot_states))) names(state_cols) \u0026lt;- sample(plot_states) update_date_anno \u0026lt;- paste(\u0026#39;Data updated on\u0026#39;, format(max(covid_county$date), \u0026#39;%d-%b-%Y\u0026#39;)) rural_point_plot \u0026lt;- covid_county_rural %\u0026gt;% left_join(tibble(state = state.name, abb = state.abb)) %\u0026gt;% ggplot(aes(tot_cases, tot_deaths))+ geom_text(aes(label = abb, color = state), size = 5, show.legend = F)+ scale_x_log10(labels = comma)+ scale_y_log10(labels = comma)+ scale_fill_manual(values = state_cols)+ scale_color_manual(values = state_cols)+ post_theme()+ labs(title = \u0026#39;Comparing Covid-19 Cases and Deaths Across Rural Counties in the U.S.\u0026#39;, subtitle = \u0026#39;SOURCE: The New York Times, based on reports from state and local health agencies \u0026amp;\\nThe USDA Rural-Urban Continuum Codes (2013).\u0026#39;, x = \u0026#39;Total Cases\u0026#39;, y = \u0026#39;Total Deaths\u0026#39;, caption = paste0(\u0026#39;Only counties shown with more than 10 cases and at least 1 death recorded.\\nNOTE:Horizontal and vertical axes are on log10 scales.\\n\u0026#39;,update_date_anno)) rural_point_plot + theme(text = element_text(size = 15))+ annotate(\u0026#39;label\u0026#39;, x = 5000, y = 5, label = str_wrap(\u0026#39;Letters are state abbreviations. Each pair represents a different county within the listed state. State abbreviations are consistently colored.\u0026#39;, 40), size = 3.5, family = \u0026#39;serif\u0026#39;, hjust = 0.5)  Key Findings It looks like, at least on the surface of things, that rural places in southern counties report more Covid-19 cases than other rural counties across the U.S.. When we investigated this further by breaking down the states within the southern category, we can see that there is somewhat even spread. That is, one state is not pulling the rest along, which could obscure the actual trend.\nI’m from the south-east of the U.S. (Mississippi), and I can say with some certainty that Mississippi, Alabama and Georgia have quite high levels of poverty and ill-health that usually accompanies poverty in the U.S.. It is interesting to see these states reporting higher levels of Covid-19, and it begs the question, “Is there an association between poorer rural places and higher levels of reported cases?” Statistically, I think there probably is (though this needs testing). Even if it is found to be statistically correlated, we still won’t know why we’re seeing these trends until we do more qualitative work in the region to pair with our quantitative findings.\nThanks to the hard work of the folks at the New York Times, we can update our findings over time to see if the trends hold. So stay tuned!\n ","date":1587859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587859200,"objectID":"e1dea9f294b4a780d9508903d70ccf9d","permalink":"/post/covid-19-rural-deltanomics/","publishdate":"2020-04-26T00:00:00Z","relpermalink":"/post/covid-19-rural-deltanomics/","section":"post","summary":"First published on 26-April-2020  Last updated on 16-Aug-2020  Introduction Now that we are square in the middle of the Covid-19 pandemic, I thought it might be beneficial to look at some statistics associated with the number of cases. We’ll differentiate our analysis by focusing on cases of Covid-19 in rural areas of the U.S. There are a couple of reasons for this: mainly, rural analytics is my speciality, so while I don’t know much about the virus, I do know some about rural societies and economies; we can easily find pertinent data on rural counties; and, we can utilise some cool built-in R functions to help us along the way.","tags":["Data","ggplot","dplyr"],"title":"Covid-19 and Rural Areas in the U.S.","type":"post"},{"authors":[],"categories":["R","SNA"],"content":"  Why create a new dataset? I’d like to do a series of posts looking at social network analysis using primary data (i.e. data collected by yourself.). There are a lot of different examples of when you might want to use a survey to collect data for use in analysing social networks. But that’s for another time.\nThe purpose of this post is to create a new dataset that can be used in practising social network analysis in future posts. Creating a new dataset in R has a lot of useful advantages. The biggest advantage is that we will have a single dataset that can be used in all future examples when learning SNA with surveys.\nCreating a new dataset is also a great learning opportunity because we will reverse engineer a dataset around specific modelling, correlations and otherwise interesting easter-eggs that we can use as learning opportunities in future posts. We will rely on the power of probability statistics to help us get there. And as we make decisions about how to structure our dataset, we’ll learn some important aspects of social network analysis and general data science. We’ll save this for the end though. So, let’s get started!\n Building a new dataset As with most posts on Deltanomics, we’ll use a tidy framework. So, that means loading tidyverse, and we’ll go ahead and load our other SNA workhorse packages.\n# For a tidy framework library(tidyverse) library(glue) library(scales) # Our graphing libraries library(igraph) library(tidygraph) library(ggraph) An edgelist The first thing we need to do is create an edgelist structure in our data. Really anything can be used as an edgelist as it’s just two columns that represent an edge is meant to be drawn between adjacent cells. A typical use of surveys in SNA is to look at how information flows between two people and the influence that the information has on sustainable behaviours. So let’s create two columns that would reasonably collect that type of information.\nRespondent name First, we need a column for the respondent’s name or identification. This column length will be the first and primary argument in our function to allow us to create datasets of any size we choose.\nFor this, let’s use one of my favourite packages randomNames to generate some realistic names.\nlibrary(randomNames) create_sna_data \u0026lt;- tibble( # let\u0026#39;s pull 100 random names to start resp_name = randomNames(100, which.names = \u0026#39;both\u0026#39;))  Information holder Next, we’ll create a column that holds the name of whom the respondent goes to for information. We want our social network to be complete; meaning that every node in the graph will attribute data. To ensure this happens, we need to take special care that all of the possible nodes are also respondents. In short, the second column of the edgelist needs to be completely contained within the first.\n# 1. Make a disconnect graph g \u0026lt;- make_empty_graph() %\u0026gt;% add_vertices(2) # 2. Run a while loop to ensure that a connected # graph is created -- this will help smooth over some of the graphing functions for later. # while (is.connected(g)== FALSE) { g \u0026lt;- create_sna_data %\u0026gt;% mutate(info_one = sample( sample(resp_name, 80), # create 2nd column nrow(.), T)) %\u0026gt;% # as subset of the as_tbl_graph() # first. } # send it back to the original name create_sna_data \u0026lt;- g Now, let’s take a look at how the social network contained within the data looks like. The network should loosely resemble a sparsely connected sociogram, and it should serve our purposes well.\n  Node \u0026amp; edge attributes Now that we have our edge list as the first two columns of the data set, we can start to add some node and edge attributes. However, we can’t just randomly create new variables and values because we want a dataset that resembles what we might find in the real world. This means certain variables should be related or correlated with one another. And, because we’re interested in network analysis, a node’s position in the network should also influence their values in key columns. To achieve this, we’ll need to reverse engineer the values based on some graph analysis.\nNode attributes We’ll do some rapid-fire node correlations with some key socio-economic variables.\nIncome category create_sna_data \u0026lt;- create_sna_data %\u0026gt;% mutate(income_pre_tax = map_chr(degree(create_sna_data), function(x){ # random normal using degree as the mean # and a standard deviation of 2.5 random_norm \u0026lt;- rnorm(n = 1, mean = x, sd = sample(2.5, 1, F)) dollar(abs(random_norm)*15000, prefix = \u0026#39;£\u0026#39;) })) Our dataset has a lot of randomness to it, so it’s impossible to tell what the correlation is. But, it should at least be positive and somewhat linear. There aren’t likely to be many nodes that have the maximum number of degrees, so the variance should drop off as the degree increases (but this isn’t a guarantee!).\nA boxplot of showing degree and income is shown below.\nSo, the theoretical people in our dataset with more connections to others should make more money, something that, could conceivably be true.\n Neighbourhood influence A common question in network analysis is: do nodes behave differently when they are connected to certain nodes. It’s like the old adage ~ if you lie down with dogs you’ll get up with fleas. For this, we’ll pick out some random nodes and have their neighbourhoods adopt a similar value for a question like: do you buy the majority of your fruit and veg from a farmers market?\ninfluencers_df \u0026lt;- map_df(1:10, function(x){ # pull a random node name node. \u0026lt;- sample(V(create_sna_data)$name, 1) # get the node id, because to_local_neighborhood requires a numeric identifier (this is due to igraph). node_id. \u0026lt;- match(node., V(create_sna_data)$name) # pull the neighbourhoods of each node from above. neighbours. \u0026lt;- create_sna_data %\u0026gt;% to_local_neighborhood(node = node_id., order = 1) %\u0026gt;% .[[1]] %\u0026gt;% as_tibble() %\u0026gt;% pull(name) # create a tibble of both values for use in the next step tibble(neighours. = neighbours., centre = rep(node., length(neighbours.))) }) # create new variable for each value returned above. create_sna_data \u0026lt;- create_sna_data %\u0026gt;% mutate(buy_farm_mark = case_when( name %in% influencers_df$centre ~ \u0026#39;Every meal\u0026#39;, name %in% influencers_df$neighours. ~ \u0026#39;Most meals\u0026#39;, T ~ \u0026#39;Hardly any meals\u0026#39; ), buy_farm_mark = factor(buy_farm_mark, levels = c(\u0026#39;Every meal\u0026#39;, \u0026#39;Most meals\u0026#39;, \u0026#39;Hardly any meals\u0026#39;))) That was a bit verbose and somewhat complicated, but it will be worth it. Let’s take a look below to see how it looks in our new data.  Community influence We’ll use a community detection algorithm for the last node attribute for our dataset. This one is a bit easier as we’ll just create a new variable using the group_infomap function from tidygraph/igraph.\ncreate_sna_data \u0026lt;- create_sna_data %\u0026gt;% to_undirected() %\u0026gt;% mutate(cows_on_farm = as.factor(group_infomap())) The plot below illustrates the communities detected by group_infomap. The only thing we’ve done here is to rename the variable. Easy enough! We’ll now add edge attributes.\n  Edge attributes Edge attributes won’t be as complicated as node attributes for as we’ve aleady identified the relationship between nodes (edges). We’ll just need to think about a variable that would makes sense for trustful communities. One could be that number of cows is related to higher levels of trust (not super likely in the real world, but anything’s possible!). It’s an easy edge attribute to calculate so let’s do that one.\ncreate_sna_data \u0026lt;- create_sna_data %\u0026gt;% mutate(trust_score = round( rescale( as.numeric(cows_on_farm), c(1, 10))))   Back to a tibble We’ve been workig with a tidygraph object for most the post. We’ll want to create a tibble for our purposes. Remember, the goal is to create a mock survey dataset that we can use in the future to learn SNA. So it should look authentic. Let’s do that now.\nname_id_df \u0026lt;- create_sna_data %\u0026gt;% as_tibble() %\u0026gt;% transmute(name, value = row_number()) create_sna_data \u0026lt;- create_sna_data %\u0026gt;% activate(edges) %\u0026gt;% as_tibble() %\u0026gt;% gather(key, value) %\u0026gt;% left_join(name_id_df) %\u0026gt;% split(.$key) %\u0026gt;% bind_cols() %\u0026gt;% select(resp_name = name, recieve_info = name1) %\u0026gt;% bind_cols(create_sna_data %\u0026gt;% as_tibble() %\u0026gt;% select(-name)) All right, that’s it! We can look at our data below; hopefully, it looks like something we might collect in the future for SNA research.\n Table 1: Our mock dataset for SNA    resp_name  recieve_info  income_pre_tax  buy_farm_mark  cows_on_farm  trust_score      Roberts, Nicole  el-Younis, Tayyiba  £6,389.69  Hardly any meals  10  6    Dixon, Lanasia  Stirewalt, Sutter  £23,319.29  Every meal  1  1    Warat, Calvin  Richardson, Chelsea  £27,098.81  Most meals  11  6    Chroneos, Samuel  Ocampo, Ruth  £26,924.22  Hardly any meals  7  4    Lamichhane, Wesley  al-Shaheen, Husaam  £36,799.65  Hardly any meals  3  2    Loehr, Jamie  Schmalz, Keiley  £73,347.91  Most meals  5  3       ","date":1585326307,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585326307,"objectID":"93453da59df3793d7c3cdefe58bb7ecd","permalink":"/post/networks-from-survey-data-creating-mock-data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/networks-from-survey-data-creating-mock-data/","section":"post","summary":"Why create a new dataset? I’d like to do a series of posts looking at social network analysis using primary data (i.e. data collected by yourself.). There are a lot of different examples of when you might want to use a survey to collect data for use in analysing social networks. But that’s for another time.\nThe purpose of this post is to create a new dataset that can be used in practising social network analysis in future posts.","tags":["Data","network"],"title":"Networks from survey data: Creating mock data","type":"post"},{"authors":["Elliot Meador PhD"],"categories":null,"content":"","date":1582329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582329600,"objectID":"3bf08d7a816aacb5144e774a34cdb423","permalink":"/talk/nbcp/","publishdate":"2020-02-22T00:00:00Z","relpermalink":"/talk/nbcp/","section":"talk","summary":"A talk on findings from a one year investigation of where people get their plant health information online in Scotland.","tags":null,"title":"Crop Production in Northern Britian","type":"talk"},{"authors":null,"categories":["R","SNA"],"content":"   Introduction  This is the initial Deltanomics blog post. So, in this post, I’ll cover a few different approaches to analysis and data visualisation rather quickly that provides a good overview of the types of things covered in this blog.\nLet’s start with loading the packages we’ll use. Also, let’s create a ggplot theme that allows us to easily make changes when we want.\n## Libraries used in analysis library(tidyverse) library(magrittr) library(scales) library(RColorBrewer) library(janitor) library(ggraph) library(tidygraph) library(graphlayouts) library(flextable) ## a congruent theme throughout for plots post_theme \u0026lt;- function(...){ theme(text = element_text(color = \u0026#39;black\u0026#39;, family = \u0026#39;serif\u0026#39;), axis.text = element_text(color = \u0026#39;black\u0026#39;), panel.background = element_blank(), axis.line.x = element_line(color = \u0026#39;black\u0026#39;), axis.ticks = element_blank(), plot.margin = margin(.5, .5, .5, .5, \u0026#39;cm\u0026#39;), plot.caption = element_text(hjust = 0, face= \u0026quot;italic\u0026quot;), plot.title = element_text(face = \u0026#39;bold\u0026#39;), plot.subtitle = element_text(face = \u0026#39;bold\u0026#39;), plot.title.position = \u0026quot;plot\u0026quot;, plot.caption.position = \u0026quot;plot\u0026quot;) + theme(...) # this bit allows us to make changes using this same function instead of calling two theme functions. }  Data source  We’re going to look at some FAO data on apples. It comes from the FAO’s online data portal, which can be accessed here. The website allows users to specify the varibles they want to analysis and download them into a .csv file. This makes working with the data a breeze using the tidyverse. Let’s first take a quick look at the data.\nWe use read_csv from the readr package (included in the tidyverse library) and the function clean_names from the janitor package. clean_names does exactly what it says it does – cleans up a dataframe/tibbles’s variable names so that they are easy to use in analysis.\napples \u0026lt;- read_csv(\u0026#39;/Users/emeador/Downloads/FAOSTAT_data_1-1-2020.csv\u0026#39;) %\u0026gt;% clean_names()  Data Analysis   General analysis  Let’s do some really quick data analysis to get a feel of what the data works with. From there we’ll move on towards looking at apple supply chain. A quick bar plot shows the top 20 exporting countries.\napple_export_total \u0026lt;- apples %\u0026gt;% filter(element == \u0026#39;Export Quantity\u0026#39;, flag_description == \u0026#39;Official data\u0026#39;)%\u0026gt;% group_by(reporter_countries) %\u0026gt;% summarise(total = sum(value, na.rm = T)) %\u0026gt;% mutate(reporter_countries=fct_reorder(reporter_countries,total)) apple_export_total %\u0026gt;% top_n(20) %\u0026gt;% ggplot(aes(reporter_countries, total))+ geom_col(fill = \u0026#39;#ff0800\u0026#39;)+ scale_y_continuous(expand = c(0,0), labels = comma)+ coord_flip()+ post_theme()+ labs(title = \u0026#39;Top 20 countries that export apples in 2017\u0026#39;, subtitle = \u0026#39; SOURCE: FAO Detailed trade matrix\u0026#39;, x = NULL, y = \u0026#39;Tonnes\u0026#39;, caption = \u0026#39;NOTE: Only official data shown\u0026#39;) China, mainland is the highest exporter of tonnes of apples in 2017 according to the data with 1,328,374 tonnes of apples exported.\nLet’s take a look at the top importers of apples to the UK. We can adapt the code above to create a bar plot that filters by the variable parter_countries, which we’ll set to filter for United Kingdom using the == operator.\nUK_import \u0026lt;- apples %\u0026gt;% filter(element == \u0026#39;Export Quantity\u0026#39;, partner_countries == \u0026#39;United Kingdom\u0026#39;, flag_description == \u0026#39;Official data\u0026#39;) %\u0026gt;% group_by(reporter_countries) %\u0026gt;% summarise(total = sum(value, na.rm = T)) %\u0026gt;% mutate(reporter_countries = fct_reorder(reporter_countries, total)) %\u0026gt;% top_n(20) UK_import %\u0026gt;% ggplot(aes(reporter_countries, total))+ geom_col(fill = \u0026#39;#ff0800\u0026#39;)+ scale_y_log10(expand = c(0,0), labels = comma)+ coord_flip()+ post_theme()+ labs(title = \u0026#39;Where does the UK get its apples from?\u0026#39;, subtitle = \u0026#39; SOURCE: FAO Detailed trade matrix\u0026#39;, x = NULL, y = \u0026#39;log10(Tonnes)\u0026#39;, caption = \u0026#39;NOTE: Only official data shown\u0026#39;)  The UK imported 444,906 tonnes of apples in 2017. Over a quarter of all apples imported to the UK came from France and other European countries. So they didn’t have to travel too far. However, the largest and third largest imports came from South Africa and New Zealand, i.e. they traveled halfway across the world!\nOf course, it’s common for goods to travel great distances in today’s global economy. This of course impacts sustainability as traveling across the world increases the carbon output. And while a total carbon assessment is out of the scope of this post, we can use network analysis to help better our understanding of how the global apple supply chain operates and where the UK sits in it all.\n Network analysis  We need to create an igraph object in R from our apples tibble to work with. The easiest way to do this is to create an edgelist from our data. An edgelist is a two-column list of nodes where adjacent nodes form an edge. igraph and by extension tidygraph will create graphs with dataframes that an edgelist in their first two columns. The remaining columns will be used as edge attributes.\nTable 1: Edgelist examplereporter_countries\npartner_countries\nGreece\nBahrain\nGermany\nCyprus\nIreland\nAustralia\nPoland\nArmenia\nBrunei Darussalam\nJapan\nEgypt\nTurkey\nAustria\nMalta\nPoland\nRepublic of Moldova\nLebanon\nNew Zealand\nDenmark\nIreland\n An example of the edgelist format is shown above in Table 1. In this example Greece is adjacent to Bahrain, so an edge will be drawn between the two.\nThe code below creates a graph and plots it using ggraph.\naph \u0026lt;- apples %\u0026gt;% filter(element == \u0026#39;Export Quantity\u0026#39;, flag_description == \u0026#39;Official data\u0026#39;) %\u0026gt;% select(reporter_countries, partner_countries, value) %\u0026gt;% as_tbl_graph() aph %\u0026gt;% mutate(degree = centrality_degree()) %\u0026gt;% ggraph(\u0026#39;stress\u0026#39;)+ # specify the DH layout geom_edge_fan(aes(alpha = ..index..), color = \u0026#39;#654321\u0026#39;, show.legend = F)+ geom_node_point(aes(size = degree), color = \u0026#39;#00c400\u0026#39;)+ scale_size(range = c(1, 2.5), name = \u0026#39;# different countries\\n that exporting apples\u0026#39;)+ coord_equal()+ theme_graph(foreground = T)+ labs(title = \u0026#39;The apple supply chain ... hairball\u0026#39;, caption = \u0026#39;Without much formatting, the network is complicated and unreadable.\\nThe look is characteristic of the hairball that sometimes occurs in network visualisations.\u0026#39;) The graph above is utterly unintelligable, and shoudn’t really appear in something you plan to publish. There are few things we can do to make the graph easier to understand when visualised. They are:\nRemove unnessary edges – this serves a few purposes: it frees up some of the clutter that comes from having too many lines on the plot; but, another lesser known thing is that it actually affects the underlying layout algorythim We’ll get into this in another post, but, in short, layout algorythims (usually) attempt to group nodes together in a way that reduces overlapping edges. Fewer edges can mean the nodes are spaced in a way so that naturally occuring patterns in connectivity are more easily seen. Identify and showcase interesting patterns – network graphs are often made better when they illustrate specific patterns that a researcher has previously identified through visualising the data or running statistical analysis. This is similar to plotting percents or sums using bars graphs – you choose the plot style (think geom_*’s in ggplot2) that corresponds to what you want to showcase!  The following code creates an edgelist in the form of a tibble that has each county's top 2 exporting countries (the two countries where it send the most apples). This greatly reduces the number of edges and allows more nuanced findings in terms of apple trading patterns to emerge.\nUK_neighborhood_1 \u0026lt;- aph %\u0026gt;% to_local_neighborhood(node = 85, order = 1, mode = \u0026#39;in\u0026#39;)%\u0026gt;% .[[1]] %\u0026gt;% activate(edges) %\u0026gt;% group_by(from) %\u0026gt;% top_n(2, value) %\u0026gt;% activate(nodes) %\u0026gt;% mutate(degree = centrality_degree()) UK_neighborhood_1 %\u0026gt;% ggraph()+ geom_edge_fan(aes(alpha = value), color = \u0026#39;#654321\u0026#39;, width = 1.25, arrow = arrow(length = unit(2.5, \u0026#39;mm\u0026#39;), type = \u0026#39;closed\u0026#39;), end_cap = circle(5, \u0026#39;mm\u0026#39;))+ geom_node_label(aes(size = degree, label = str_wrap(name, 10)), color = \u0026#39;#ff0800\u0026#39;, show.legend = F)+ scale_size(range = c(2, 3))+ scale_edge_alpha(range = c(.5, 1), labels = comma)+ scale_edge_width_continuous(range = c(.5, 1.5))+ coord_equal()+ theme_graph(foreground = T)+ labs(title = \u0026#39;The UK\\\u0026#39;s 1-degree apple supply neighborhood\u0026#39;, caption = \u0026#39;Only the top-two export destinations are shown per country. All nodes have exactly two out-degrees.\\nSome nodes have in-degrees because they happen to be another country\\\u0026#39;s top-two imports.\u0026#39;)  Visualising travel distance   A quick base-map of the world  We can draw on existing online resources to help us prepare a base map of the world using ggplot and a (newish) file type called simple features sf. Here is a great resource on mapping and spatial analysis in R using ggplot2 by Mel Moreno and Mathieu Basille. I highly recommend checking it out. sf are my prefered object types to work with in R when doing any type of mapping or spatial analysis. The map is projected using the Equal Earth projection to help readers more easily see the network edges (when they are plotted).\nLet’s take a look at a world map that we can use as a base for the network plot. We’ll use the rnaturalearth and rnaturalearthdata packages to provde parameters and data as sf objects, and we’ll plot the map in ggplot2. ggplot2 and ggraph can objects can be stacked on top of one another to create a flowing network map.\nlibrary(rnaturalearth) library(rnaturalearthdata) countries \u0026lt;- ne_countries(returnclass = \u0026quot;sf\u0026quot;) graticules \u0026lt;- ne_download(type = \u0026quot;graticules_15\u0026quot;, category = \u0026quot;physical\u0026quot;, returnclass = \u0026quot;sf\u0026quot;) ## OGR data source with driver: ESRI Shapefile ## Source: \u0026quot;/private/var/folders/ck/v11m55r567d9z7ql_1vvdy600000gn/T/Rtmpb9TTSG\u0026quot;, layer: \u0026quot;ne_110m_graticules_15\u0026quot; ## with 35 features ## It has 5 fields ## Integer64 fields read as strings: degrees scalerank bound_box \u0026lt;- ne_download(type = \u0026quot;wgs84_bounding_box\u0026quot;, category = \u0026quot;physical\u0026quot;, returnclass = \u0026quot;sf\u0026quot;) ## OGR data source with driver: ESRI Shapefile ## Source: \u0026quot;/private/var/folders/ck/v11m55r567d9z7ql_1vvdy600000gn/T/Rtmpb9TTSG\u0026quot;, layer: \u0026quot;ne_110m_wgs84_bounding_box\u0026quot; ## with 1 features ## It has 2 fields (base_world \u0026lt;- ggplot() + geom_sf(data = bound_box, col = \u0026quot;grey20\u0026quot;, fill = \u0026quot;transparent\u0026quot;) + geom_sf(data = countries, aes(fill = sovereignt), color = \u0026#39;grey\u0026#39;, lwd = 0.3, show.legend = F) + scale_fill_viridis_d(direction = -1)+ post_theme(legend.position = \u0026#39;bottom\u0026#39;, legend.background = element_rect(fill = \u0026#39;grey95\u0026#39;, color = \u0026#39;black\u0026#39;))+ theme(plot.title = element_text(size = 24, face = \u0026#39;bold\u0026#39;), axis.text = element_blank())+ labs(title = \u0026#39;World Map\u0026#39;, caption = \u0026#39;Projected with the Equal Earth map projection \u0026#39;))  Combining the basemap and network graph  In preparation for our supply chain we need to calculate the node positions for each country. A good starting point is to use a polygon’s centroid points. A polygon centroid is the mathmatical centre of mass. Which means that it’s slightly different that the mean of longitude and latitude. The unique and non-uniform shapes of most policital boundaries mean that centre-mass locations are usually preferred. We can use the ‘st_centroid’ function from the ‘sf’ package to calculate the centroids for every country in the world. We’ll save this as ‘country_centroids’.\n# get centroids country_centroids \u0026lt;- countries %\u0026gt;% sf::st_centroid() %\u0026gt;% as_tibble() %\u0026gt;% select(name, geometry) %\u0026gt;% mutate(geometry = as.character(geometry)) %\u0026gt;% separate(geometry, c(\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;), sep = \u0026#39;,\u0026#39;) %\u0026gt;% mutate_at(vars(x, y), list(~parse_number(.))) # a little cleaning of a few countries to # ensure that they merge properly. node_centroids \u0026lt;- UK_neighborhood_1 %\u0026gt;% as_tibble() %\u0026gt;% select(name) %\u0026gt;% mutate(name = case_when( str_detect(name, \u0026#39;China\u0026#39;) ~ \u0026#39;China\u0026#39;, str_detect(name, \u0026#39;Iran\u0026#39;) ~ \u0026#39;Iran\u0026#39;, str_detect(name, \u0026#39;Czechia\u0026#39;) ~ \u0026#39;Czech Rep.\u0026#39;, str_detect(name, \u0026#39;United States of America\u0026#39;) ~ \u0026#39;United States\u0026#39;, T~name )) %\u0026gt;% left_join(country_centroids) layout_centroid \u0026lt;- node_centroids %\u0026gt;% select(-name) Finally, we’ll use ggraph to make the final plot. We use a layered approach and add some geom_sf’s to input the background world map.\n# start with a ggraph ggraph(UK_neighborhood_1, layout = layout_centroid)+ geom_sf(data = bound_box, col = \u0026quot;grey20\u0026quot;, fill = \u0026quot;transparent\u0026quot;) + geom_sf(data = countries, ## add the geom_sf to map aes(fill = sovereignt), color = \u0026#39;grey\u0026#39;, lwd = 0.3, show.legend = F)+ geom_edge_arc(arrow = arrow(type = \u0026#39;closed\u0026#39;, # add geom_edge for edges length = unit(1, \u0026#39;mm\u0026#39;)), width = .75, color = \u0026#39;black\u0026#39;, end_cap = circle(1.25, \u0026#39;mm\u0026#39;), alpha = .75, strength = .15)+ post_theme(legend.position = \u0026#39;bottom\u0026#39;, legend.background = element_rect(fill = \u0026#39;grey95\u0026#39;, color = \u0026#39;black\u0026#39;))+ scale_fill_viridis_d(direction = -1)+ theme(plot.title = element_text(size = 24, face = \u0026#39;bold\u0026#39;), axis.text = element_blank())+ labs(title = \u0026#39;Apples for apples\u0026#39;, subtitle = \u0026#39;UK first degree network\u0026#39;, caption = \u0026#39;Projected with the Equal Earth map projection \u0026#39;) ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"904ac0ce1ccf714d0aa0e68661b9c0fc","permalink":"/post/apples-for-apples/quick-start-network-analysis/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/post/apples-for-apples/quick-start-network-analysis/","section":"post","summary":"Introduction  This is the initial Deltanomics blog post. So, in this post, I’ll cover a few different approaches to analysis and data visualisation rather quickly that provides a good overview of the types of things covered in this blog.\nLet’s start with loading the packages we’ll use. Also, let’s create a ggplot theme that allows us to easily make changes when we want.\n## Libraries used in analysis library(tidyverse) library(magrittr) library(scales) library(RColorBrewer) library(janitor) library(ggraph) library(tidygraph) library(graphlayouts) library(flextable) ## a congruent theme throughout for plots post_theme \u0026lt;- function(.","tags":["ggraph","tidygraph"],"title":"Apples for apples I","type":"post"},{"authors":["Marianna Markantoni, PhD","Artur Adam Steiner, PhD","Elliot Meador PhD"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"6c5835d02b4a7645c22a7b880d9c8f27","permalink":"/publication/community-resilience-2019/index/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/community-resilience-2019/index/","section":"publication","summary":"Quantitative study of rural community resilience in southwest Scotland.","tags":["Source Themes"],"title":"Can community interventions change resilience? Fostering perceptions of individual and community resilience in rural places","type":"publication"},{"authors":["Elliot Meador PhD","David O'Brien"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"e97bc220bb71e7c77a88a4c3ffcfe55a","permalink":"/publication/food-security-rwanda/index/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/food-security-rwanda/index/","section":"publication","summary":"Trust in one's cooperative leadership is most important in predicting the likelihood that someone recommends their coopertive to their neighbourhood.","tags":["Source Themes"],"title":"Placing Rwanda’s agriculture boom: trust, women empowerment and policy impact in maize agricultural cooperatives","type":"publication"},{"authors":["Elliot Meador PhD"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"e8c252376302d7f5ce7c3df3ba6c8af8","permalink":"/publication/reaching-rural/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/reaching-rural/","section":"publication","summary":"Social network analysis of inter-connected board members in Missouri suggests that networks do not purmeate into rural areas of the state.","tags":["Source Themes"],"title":"Reaching rural: Identifying implicit social networks in community development programmes","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"f40150aeb56bb9e479dd7519951a1a85","permalink":"/project/project-tgrains/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/project-tgrains/","section":"project","summary":"This project seeks to build briding social capital in the UK food system.","tags":["Demo"],"title":"Transforming and Growing Relationships within regionAl food systems for Improved Nutrition and Sustainability","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bdd8e854d486c6ae4d1298f625100dba","permalink":"/links/r-bloggers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/links/r-bloggers/","section":"links","summary":"","tags":null,"title":"","type":"links"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ce289b0fc7731b36956efe8474810a5f","permalink":"/post/d1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/d1/","section":"post","summary":"","tags":null,"title":"","type":"post"}]