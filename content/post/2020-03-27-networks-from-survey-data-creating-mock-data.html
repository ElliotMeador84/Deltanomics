---
title: 'Networks from survey data: Creating mock data'
author: Elliot Meador
date: 
slug: networks-from-survey-data-creating-mock-data
categories:
  - R
  - SNA
tags:
  - Data
  - network
subtitle: ''
summary: ''
authors: []
lastmod: '2020-03-27T16:25:07Z'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
editor_options: 
  chunk_output_type: console
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/datatables-binding/datatables.js"></script>
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>


<div id="why-create-a-new-dataset" class="section level1">
<h1>Why create a new dataset?</h1>
<p>I’d like to do a series of posts looking at social network analysis using primary data (i.e. data collected by yourself.).
There are a lot of different examples of when you might want to use a survey to collect data for use in analysing social networks.
But that’s for another time.</p>
<p>The purpose of this post is to create a new dataset that can be used in practising social network analysis in future posts.
Creating a new dataset in R has a lot of useful advantages.
The biggest advantage is that we will have a single dataset that can be used in all future examples when learning SNA with surveys.</p>
<p>Creating a new dataset is also a great learning opportunity because we will <em>reverse engineer</em> a dataset around specific modelling, correlations and otherwise interesting easter-eggs that we can use as learning opportunities in future posts.
We will rely on the power of probability statistics to help us get there.
And as we make decisions about how to structure our dataset, we’ll learn some important aspects of social network analysis and general data science.
We’ll save this for the end though.
So, let’s get started!</p>
</div>
<div id="building-a-new-dataset" class="section level1">
<h1>Building a new dataset</h1>
<p>As with most posts on Deltanomics, we’ll use a tidy framework. So, that means loading <code>tidyverse</code>, and we’ll go ahead and load our other SNA workhorse packages.</p>
<pre class="r"><code># For a tidy framework
library(tidyverse)
library(glue)
library(scales)

# Our graphing libraries
library(igraph)
library(tidygraph)
library(ggraph)</code></pre>
<div id="an-edgelist" class="section level2">
<h2>An edgelist</h2>
<p>The first thing we need to do is create an <a href="https://en.wikipedia.org/wiki/Edge_list">edgelist</a> structure in our data.
Really anything can be used as an edgelist as it’s just two columns that represent an edge is meant to be drawn between adjacent cells.
A typical use of surveys in SNA is to look at how information flows between two people and the influence that the information has on sustainable behaviours.
So let’s create two columns that would reasonably collect that type of information.</p>
<div id="respondent-name" class="section level3">
<h3>Respondent name</h3>
<p>First, we need a column for the respondent’s name or identification.
This column length will be the first and primary argument in our function to allow us to create datasets of any size we choose.</p>
<p>For this, let’s use one of my favourite packages <code>randomNames</code> to generate some realistic names.</p>
<pre class="r"><code>library(randomNames)

create_sna_data &lt;- 
tibble( # let&#39;s pull 100 random names to start
    resp_name = randomNames(100,
                  which.names = &#39;both&#39;))</code></pre>
</div>
<div id="information-holder" class="section level3">
<h3>Information holder</h3>
<p>Next, we’ll create a column that holds the name of whom the respondent goes to for information.
We want our social network to be <em>complete</em>; meaning that every node in the graph will attribute data.
To ensure this happens, we need to take special care that all of the possible nodes are also respondents.
In short, the second column of the edgelist needs to be completely contained within the first.</p>
<pre class="r"><code># 1. Make a disconnect graph
g &lt;- make_empty_graph() %&gt;% 
  add_vertices(2)

# 2. Run a while loop to ensure that a connected
# graph is created -- this will help smooth over some of the graphing functions for later. 
# 
while (is.connected(g)== FALSE) {

  g &lt;- create_sna_data %&gt;% 
    mutate(info_one = sample(
      sample(resp_name, 80), # create 2nd column
      nrow(.), T)) %&gt;%       # as subset of the
    as_tbl_graph()           # first.
}

# send it back to the original name
create_sna_data &lt;-  g</code></pre>
<p>Now, let’s take a look at how the social network contained within the data looks like.
<img src="/post/2020-03-27-networks-from-survey-data-creating-mock-data_files/figure-html/unnamed-chunk-4-1.png" width="960" />
The network should loosely resemble a sparsely connected sociogram, and it should serve our purposes well.</p>
</div>
</div>
<div id="node-edge-attributes" class="section level2">
<h2>Node &amp; edge attributes</h2>
<p>Now that we have our edge list as the first two columns of the data set, we can start to add some node and edge attributes. However, we can’t just randomly create new variables and values because we want a dataset that resembles what we might find in the real world. This means certain variables should be related or correlated with one another. And, because we’re interested in network analysis, a node’s position in the network should also influence their values in key columns. To achieve this, we’ll need to <em>reverse</em> engineer the values based on some graph analysis.</p>
<div id="node-attributes" class="section level3">
<h3>Node attributes</h3>
<p>We’ll do some rapid-fire node correlations with some key socio-economic variables.</p>
<div id="income-category" class="section level4">
<h4>Income category</h4>
<pre class="r"><code>create_sna_data &lt;- create_sna_data %&gt;% 
  mutate(income_pre_tax = map_chr(degree(create_sna_data), function(x){
  
    # random normal using degree as the mean
    # and a standard deviation of 2.5
    random_norm &lt;- rnorm(n = 1, 
        mean = x, 
        sd = sample(2.5, 1, F))
  
  dollar(abs(random_norm)*15000, 
         prefix = &#39;£&#39;)
}))</code></pre>
<p>Our dataset has a lot of randomness to it, so it’s impossible to tell what the correlation is. But, it should at least be positive and somewhat linear. There aren’t likely to be many nodes that have the maximum number of degrees, so the variance should drop off as the degree increases (but this isn’t a guarantee!).</p>
<p>A boxplot of showing degree and income is shown below.</p>
<p><img src="/post/2020-03-27-networks-from-survey-data-creating-mock-data_files/figure-html/unnamed-chunk-6-1.png" width="672" />
So, the theoretical people in our dataset with more connections to others should make more money, something that, could conceivably be true.</p>
</div>
<div id="neighbourhood-influence" class="section level4">
<h4>Neighbourhood influence</h4>
<p>A common question in network analysis is: do nodes behave differently when they are connected to certain nodes. It’s like the old adage ~ <em>if you lie down with dogs you’ll get up with fleas</em>. For this, we’ll pick out some random nodes and have their neighbourhoods adopt a similar value for a question like: do you buy the majority of your fruit and veg from a farmers market?</p>
<pre class="r"><code>influencers_df &lt;- map_df(1:10, function(x){

  # pull a random node name
node. &lt;- sample(V(create_sna_data)$name, 1)

  # get the node id, because to_local_neighborhood requires a numeric identifier (this is due to igraph).

 node_id. &lt;- match(node., V(create_sna_data)$name)
  
 # pull the neighbourhoods of each node from above. 
neighbours. &lt;- create_sna_data %&gt;% 
  to_local_neighborhood(node = node_id., 
                        order = 1) %&gt;% 
    .[[1]] %&gt;% 
    as_tibble() %&gt;% 
    pull(name)

 # create a tibble of both values for use in the next step

tibble(neighours. = neighbours.,
       centre = rep(node., length(neighbours.)))

}) 
# create new variable for each value returned above.

create_sna_data &lt;- 
  create_sna_data %&gt;% 
  mutate(buy_farm_mark = case_when(
    name %in% influencers_df$centre ~ &#39;Every meal&#39;,
    name %in% influencers_df$neighours. ~ &#39;Most meals&#39;,
    T ~ &#39;Hardly any meals&#39;
  ), 
  buy_farm_mark = factor(buy_farm_mark, 
                            levels = c(&#39;Every meal&#39;, 
                                       &#39;Most meals&#39;, 
                                       &#39;Hardly any meals&#39;)))</code></pre>
<p>That was a bit verbose and somewhat complicated, but it will be worth it. Let’s take a look below to see how it looks in our new data.
<img src="/post/2020-03-27-networks-from-survey-data-creating-mock-data_files/figure-html/unnamed-chunk-8-1.png" width="960" /></p>
</div>
<div id="community-influence" class="section level4">
<h4>Community influence</h4>
<p>We’ll use a community detection algorithm for the last node attribute for our dataset. This one is a bit easier as we’ll just create a new variable using the <code>group_infomap</code> function from <code>tidygraph</code>/<code>igraph</code>.</p>
<pre class="r"><code>create_sna_data &lt;- create_sna_data %&gt;% 
  to_undirected() %&gt;% 
  mutate(cows_on_farm = 
           as.factor(group_infomap()))</code></pre>
<p>The plot below illustrates the communities detected by <code>group_infomap</code>. The only thing we’ve done here is to rename the variable. Easy enough!
<img src="/post/2020-03-27-networks-from-survey-data-creating-mock-data_files/figure-html/unnamed-chunk-10-1.png" width="960" /></p>
<p>We’ll now add edge attributes.</p>
</div>
</div>
<div id="edge-attributes" class="section level3">
<h3>Edge attributes</h3>
<p>Edge attributes won’t be as complicated as node attributes for as we’ve aleady identified the relationship between nodes (edges). We’ll just need to think about a variable that would makes sense for trustful communities. One could be that number of cows is related to higher levels of trust (not super likely in the real world, but anything’s possible!). It’s an easy edge attribute to calculate so let’s do that one.</p>
<pre class="r"><code>create_sna_data &lt;- create_sna_data %&gt;% 
  mutate(trust_score = round(
              rescale(
              as.numeric(cows_on_farm), 
              c(1, 10))))</code></pre>
</div>
</div>
<div id="back-to-a-tibble" class="section level2">
<h2>Back to a tibble</h2>
<p>We’ve been workig with a <code>tidygraph</code> object for most the post. We’ll want to create a <code>tibble</code> for our purposes. Remember, the goal is to create a mock survey dataset that we can use in the future to learn SNA. So it should look authentic. Let’s do that now.</p>
<pre class="r"><code>name_id_df &lt;- create_sna_data %&gt;% 
  as_tibble() %&gt;% 
  transmute(name, 
            value = row_number())

create_sna_data &lt;- create_sna_data %&gt;% 
  activate(edges) %&gt;% 
  as_tibble() %&gt;% 
  gather(key, value) %&gt;% 
  left_join(name_id_df) %&gt;% 
  split(.$key) %&gt;% 
  bind_cols() %&gt;% 
  select(resp_name = name, 
         recieve_info = name1) %&gt;% 
  bind_cols(create_sna_data %&gt;% 
              as_tibble() %&gt;% 
              select(-name))</code></pre>
<pre><code>## Joining, by = &quot;value&quot;</code></pre>
<p>All right, that’s it! We can look at our data below; hopefully, it looks like something we might collect in the future for SNA research.</p>
<div id="htmlwidget-1" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100"],["Naple, Sara","Desta, Kadijah","Naple, Sara","Saqib, Nikhil","Spaziani, Kelly","Knoll, Katherine","Lee, Chris","Schwarz, Nicole","Schwarz, Nicole","Vu, Gaston","Shakespeare, John","Beeck, Kimberly","Green, Lia","Gardner, Kindra","Anderson, Dejane","Mead, Shailyn","Vang, Reyana","el-Soltani, Abdul Baasid","Perez-Luna, Pedro","Mcclendon, Rechael","Norwood, Donte","Hughes, Tyaisha","Saqib, Nikhil","Aragon, Adriana","Cartwright, Kassidy","el-Abu, Baahir","Burke, Baja","Burke, Baja","Camacho, Marcus","Martinez, Jasmin","Lanoue, Christine","Valdoria, Patrick","Christensen, Patrick","Roybal, Ashley","Beeck, Kimberly","Robinson, Fatima","Schwarz, Nicole","Bunting, Brieanna","Tyler, Joshua","Naple, Sara","Munoz, Cruz","Douglas, Darian","Morrow, Cassandra Lynn","Robinson, Fatima","al-Saad, Raheema","Lapaz, Thalia","al-Ghazi, Zaki","Bochmann, Jacob","Salh, Oh","Beeck, Kimberly","Lee, Chris","Polk, Kristina","Schwarz, Nicole","Polk, Kristina","Valdoria, Patrick","Saqib, Nikhil","Schwarz, Nicole","Salazar, Maria","Naple, Sara","Hughes, Tyaisha","Shakespeare, John","Guill, Sierra","Phan, Philip","Beegle, Taylor","Polk, Kristina","Gardner, Kindra","Anderson, Dejane","Auyeung, Sovanara","el-Soltani, Abdul Baasid","Naple, Sara","Brauher, Eli","Knoll, Katherine","Humrich, Jeidyn","Trollan, Jordan","Vasquez, Mishael","Martinez, Jasmin","Mead, Shailyn","Norwood, Donte","Burke, Baja","Stamey-Carter, Miriah","Salh, Oh","Mcclendon, Rechael","Cook, Martin","Salazar, Maria","Hughes, Tyaisha","Mead, Shailyn","Sahani, Jessica","Abbott, Patrick","Hughes, Tyaisha","Murray, Rafael","Salazar, Maria","el-Zamani, Waseef","Mccabe, Deisha","Jolly, Somarrey","Salazar, Maria","Zamora, Bridget","el-Zamani, Waseef","Phan, Philip","Phan, Philip","Caldwell, Tessa"],["Polk, Kristina","Polk, Kristina","Ospina, Henry","Naple, Julia","Shakespeare, John","Robinson, Fatima","Schwarz, Nicole","Abbott, Patrick","Hopkins, Doris","Mcclendon, Rechael","Martinez, Jasmin","Salh, Oh","Garbarini, Lynn","el-Husain, Suwailim","Cook, Martin","Steffens, Zachary","Minjares, Lauren","Curran, Donovan","Portugal, Samuel","Garbarini, Lynn","Steffens, Zachary","el-Jalali, Maleeha","Humrich, Jeidyn","Caldwell, Tessa","Salazar, Maria","Murray, Rafael","Polk, Kristina","Garza, Sheena","Mccabe, Deisha","Lapaz, Thalia","Lor, Ishan","Naple, Julia","Gonzales, Steven","Zamora, Bridget","Johnson, Brooklyn","Portugal, Samuel","Beegle, Taylor","Salh, Oh","Craig, Tiara","Asad, Sienna","Lor, Ishan","Craig, Tiara","Sahani, Jessica","Van, Ana","Salazar, Maria","el-Zamani, Waseef","Sahani, Jessica","Garbarini, Lynn","el-Zamani, Waseef","Griego, Derrick","Phetchamphone, Mackenzie","al-Karam, Labeeb","Cook, Martin","Guill, Sierra","Jolly, Somarrey","el-Zamani, Waseef","Stamey-Carter, Miriah","al-Karam, Labeeb","el-Maroun, Marwa","Clark, Kaori","al-Karam, Labeeb","Schaible, Landon","Gonzales, Steven","Curran, Donovan","Cunningham, Benjamin","al-Farra, Ameera","Brauher, Eli","Rabinoff, Matthew","Abbott, Patrick","Hopkins, Stephanie","Trollan, Jordan","al-Bahri, Aliyya","Baker, Cecilia","Ho, Christy","Mccabe, Deisha","Portugal, Samuel","Gonzales, Steven","Hill, Ashton","Parra, Gabriel","Lo, Josiah","Morgan, Zachary","Sahani, Jessica","Mccabe, Deisha","Martinez, Marina","Kirgis, Marcus","Murray, Rafael","el-Husain, Suwailim","Naple, Julia","Craig, Tiara","Caldwell, Tessa","Zamora, Bridget","Steffens, Zachary","Minjares, Lauren","Rabinoff, Matthew","Lor, Ishan","Garbarini, Lynn","Quintana, Terry","al-Beshara, Abdus Samad","el-Jalali, Maleeha","Dube, Madeline"],["£100,436","£12,441.98","£29,041.64","£66,958.24","£2,537.40","£17,455.98","£70,888.11","£90,372.82","£2,298.29","£36,674.68","£47,457.89","£55,674.13","£12,845.41","£59,066.90","£7,529.34","£34,437.52","£4,228.07","£70,674.83","£21,271.36","£83,121.08","£35,739.57","£19,649.65","£62,616.42","£22,169.12","£68,472.99","£9,437.46","£56,303.32","£5,650.88","£7,991.39","£38,890.60","£9,041.24","£26,690.09","£4,363.18","£28,923.45","£3,205.44","£50,573.79","£44,221.71","£30,675.63","£851.52","£18,802.24","£60,882.17","£22,763.02","£10,379.50","£25,473.36","£3,728.62","£74,461.31","£74,777.08","£7,972.50","£46,288.31","£9,727.12","£11,679.04","£72,526.02","£731.62","£89,316.73","£71,699.05","£53,159.00","£1,438.65","£74,243.47","£154.83","£34,587.21","£47,199.10","£56,361.32","£5,765.74","£45,309.90","£6,854.77","£17,451.79","£78,998.50","£39,669.50","£47,680.12","£2,158.98","£34,863.03","£16,427.29","£3,978.81","£25,270.95","£10,166.95","£69,725.61","£48,205.36","£17,024.81","£23,207.98","£27,980.18","£20,129.18","£100,048","£66,425.55","£34,043.80","£21,855.00","£52,296.04","£23,857.18","£59,558.35","£39,512.17","£21,624.99","£24,219.63","£90,187.91","£13,275.91","£42,767.76","£76,154.87","£6,794.70","£11,872.43","£17,258.00","£41,809.85","£15,237.35"],["Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Most meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Most meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Every meal","Hardly any meals","Every meal","Hardly any meals","Hardly any meals","Most meals","Hardly any meals","Hardly any meals","Every meal","Hardly any meals","Every meal","Hardly any meals","Most meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Every meal","Most meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Most meals","Most meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Every meal","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Most meals","Hardly any meals","Hardly any meals","Most meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Hardly any meals","Most meals","Hardly any meals","Every meal","Hardly any meals","Hardly any meals","Hardly any meals","Every meal","Most meals","Hardly any meals","Every meal","Hardly any meals","Hardly any meals"],["16","9","16","8","10","6","1","1","1","3","10","7","3","5","11","17","15","1","6","3","17","4","8","14","2","14","18","18","15","10","2","12","13","3","7","6","1","7","4","16","2","4","5","6","2","10","5","3","7","7","1","9","11","9","12","8","1","2","16","4","10","9","13","1","9","5","11","12","1","16","11","6","8","11","15","6","13","17","18","1","7","5","15","2","4","14","5","12","4","14","3","17","15","12","2","3","8","13","13","14"],[9,5,9,5,6,4,1,1,1,2,6,4,2,3,6,9,8,1,4,2,9,3,5,8,2,8,10,10,8,6,2,7,7,2,4,4,1,4,3,9,2,3,3,4,2,6,3,2,4,4,1,5,6,5,7,5,1,2,9,3,6,5,7,1,5,3,6,7,1,9,6,4,5,6,8,4,7,9,10,1,4,3,8,2,3,8,3,7,3,8,2,9,8,7,2,2,5,7,7,8]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>resp_name<\/th>\n      <th>recieve_info<\/th>\n      <th>income_pre_tax<\/th>\n      <th>buy_farm_mark<\/th>\n      <th>cows_on_farm<\/th>\n      <th>trust_score<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":6},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
